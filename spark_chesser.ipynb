{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries #\n",
    "import findspark\n",
    "# findspark.find()\n",
    "findspark.init()\n",
    "from pyspark.sql.functions import split, col, array_contains, translate, round, size, when, udf, lit, mean, count, format_number, collect_list, expr\n",
    "from pyspark.sql.types import TimestampType, MapType, IntegerType, StringType, ArrayType, FloatType, StructField, StructType\n",
    "from pyspark.sql import SparkSession\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 21:37:41,676 INFO spark.SparkContext: Running Spark version 3.3.2\n",
      "2023-04-26 21:37:42,130 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-04-26 21:37:42,132 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2023-04-26 21:37:42,132 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-04-26 21:37:42,133 INFO spark.SparkContext: Submitted application: test\n",
      "2023-04-26 21:37:42,177 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2023-04-26 21:37:42,209 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "2023-04-26 21:37:42,217 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2023-04-26 21:37:42,314 INFO spark.SecurityManager: Changing view acls to: ubuntu\n",
      "2023-04-26 21:37:42,317 INFO spark.SecurityManager: Changing modify acls to: ubuntu\n",
      "2023-04-26 21:37:42,318 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-04-26 21:37:42,319 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-04-26 21:37:42,319 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()\n",
      "2023-04-26 21:37:42,909 INFO util.Utils: Successfully started service 'sparkDriver' on port 34473.\n",
      "2023-04-26 21:37:42,970 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2023-04-26 21:37:43,032 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2023-04-26 21:37:43,069 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2023-04-26 21:37:43,071 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2023-04-26 21:37:43,132 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2023-04-26 21:37:43,183 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-64130d1c-cbfe-421c-92b2-98b60ef95f55\n",
      "2023-04-26 21:37:43,213 INFO memory.MemoryStore: MemoryStore started with capacity 93.3 MiB\n",
      "2023-04-26 21:37:43,272 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2023-04-26 21:37:43,364 INFO util.log: Logging initialized @12909ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2023-04-26 21:37:43,580 INFO server.Server: jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09\n",
      "2023-04-26 21:37:43,616 INFO server.Server: Started @13162ms\n",
      "2023-04-26 21:37:43,670 INFO server.AbstractConnector: Started ServerConnector@c78be70{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2023-04-26 21:37:43,670 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "2023-04-26 21:37:43,719 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e9aa2c7{/,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:37:44,698 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at namenode/192.168.11.141:8032\n",
      "2023-04-26 21:37:46,470 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-04-26 21:37:46,471 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-04-26 21:37:46,496 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)\n",
      "2023-04-26 21:37:46,498 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "2023-04-26 21:37:46,499 INFO yarn.Client: Setting up container launch context for our AM\n",
      "2023-04-26 21:37:46,503 INFO yarn.Client: Setting up the launch environment for our AM container\n",
      "2023-04-26 21:37:46,516 INFO yarn.Client: Preparing resources for our AM container\n",
      "2023-04-26 21:37:47,266 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "2023-04-26 21:38:16,789 INFO yarn.Client: Uploading resource file:/tmp/spark-e2673aa0-8dfe-40ce-b467-2dabb407cd68/__spark_libs__559210578876841426.zip -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1681995144617_0071/__spark_libs__559210578876841426.zip\n",
      "2023-04-26 21:38:22,636 INFO yarn.Client: Uploading resource file:/home/ubuntu/spark/python/lib/pyspark.zip -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1681995144617_0071/pyspark.zip\n",
      "2023-04-26 21:38:24,072 INFO yarn.Client: Uploading resource file:/home/ubuntu/spark/python/lib/py4j-0.10.9.5-src.zip -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1681995144617_0071/py4j-0.10.9.5-src.zip\n",
      "2023-04-26 21:38:26,182 INFO yarn.Client: Uploading resource file:/tmp/spark-e2673aa0-8dfe-40ce-b467-2dabb407cd68/__spark_conf__7763378659141478751.zip -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1681995144617_0071/__spark_conf__.zip\n",
      "2023-04-26 21:38:30,244 INFO spark.SecurityManager: Changing view acls to: ubuntu\n",
      "2023-04-26 21:38:30,246 INFO spark.SecurityManager: Changing modify acls to: ubuntu\n",
      "2023-04-26 21:38:30,246 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-04-26 21:38:30,246 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-04-26 21:38:30,247 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()\n",
      "2023-04-26 21:38:30,282 INFO yarn.Client: Submitting application application_1681995144617_0071 to ResourceManager\n",
      "2023-04-26 21:38:30,392 INFO impl.YarnClientImpl: Submitted application application_1681995144617_0071\n",
      "2023-04-26 21:38:31,398 INFO yarn.Client: Application report for application_1681995144617_0071 (state: ACCEPTED)\n",
      "2023-04-26 21:38:31,404 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Wed Apr 26 21:38:30 +0000 2023] Scheduler has assigned a container for AM, waiting for AM container to be launched\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1682545110333\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://namenode:8088/proxy/application_1681995144617_0071/\n",
      "\t user: ubuntu\n",
      "2023-04-26 21:38:32,408 INFO yarn.Client: Application report for application_1681995144617_0071 (state: ACCEPTED)\n",
      "2023-04-26 21:38:33,413 INFO yarn.Client: Application report for application_1681995144617_0071 (state: ACCEPTED)\n",
      "2023-04-26 21:38:34,417 INFO yarn.Client: Application report for application_1681995144617_0071 (state: ACCEPTED)\n",
      "2023-04-26 21:38:35,419 INFO yarn.Client: Application report for application_1681995144617_0071 (state: ACCEPTED)\n",
      "2023-04-26 21:38:36,420 INFO yarn.Client: Application report for application_1681995144617_0071 (state: ACCEPTED)\n",
      "2023-04-26 21:38:37,423 INFO yarn.Client: Application report for application_1681995144617_0071 (state: ACCEPTED)\n",
      "2023-04-26 21:38:38,426 INFO yarn.Client: Application report for application_1681995144617_0071 (state: ACCEPTED)\n",
      "2023-04-26 21:38:39,430 INFO yarn.Client: Application report for application_1681995144617_0071 (state: ACCEPTED)\n",
      "2023-04-26 21:38:40,432 INFO yarn.Client: Application report for application_1681995144617_0071 (state: ACCEPTED)\n",
      "2023-04-26 21:38:41,435 INFO yarn.Client: Application report for application_1681995144617_0071 (state: ACCEPTED)\n",
      "2023-04-26 21:38:42,438 INFO yarn.Client: Application report for application_1681995144617_0071 (state: ACCEPTED)\n",
      "2023-04-26 21:38:43,240 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> namenode, PROXY_URI_BASES -> http://namenode:8088/proxy/application_1681995144617_0071), /proxy/application_1681995144617_0071\n",
      "2023-04-26 21:38:43,441 INFO yarn.Client: Application report for application_1681995144617_0071 (state: RUNNING)\n",
      "2023-04-26 21:38:43,444 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 192.168.11.85\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1682545110333\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://namenode:8088/proxy/application_1681995144617_0071/\n",
      "\t user: ubuntu\n",
      "2023-04-26 21:38:43,448 INFO cluster.YarnClientSchedulerBackend: Application application_1681995144617_0071 has started running.\n",
      "2023-04-26 21:38:43,465 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44765.\n",
      "2023-04-26 21:38:43,465 INFO netty.NettyBlockTransferService: Server created on namenode:44765\n",
      "2023-04-26 21:38:43,468 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2023-04-26 21:38:43,487 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, namenode, 44765, None)\n",
      "2023-04-26 21:38:43,495 INFO storage.BlockManagerMasterEndpoint: Registering block manager namenode:44765 with 93.3 MiB RAM, BlockManagerId(driver, namenode, 44765, None)\n",
      "2023-04-26 21:38:43,500 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, namenode, 44765, None)\n",
      "2023-04-26 21:38:43,502 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, namenode, 44765, None)\n",
      "2023-04-26 21:38:44,110 INFO history.SingleEventLogFileWriter: Logging events to hdfs://namenode:9000/directory/application_1681995144617_0071.inprogress\n",
      "2023-04-26 21:38:45,454 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4e9aa2c7{/,null,STOPPED,@Spark}\n",
      "2023-04-26 21:38:45,464 INFO ui.ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,501 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a7e8090{/jobs,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,506 INFO ui.ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,508 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@15f9de79{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,508 INFO ui.ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,510 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d3d16e1{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,512 INFO ui.ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,513 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1db024ee{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,513 INFO ui.ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,514 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3c48fec8{/stages,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,515 INFO ui.ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,516 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fd420e5{/stages/json,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,516 INFO ui.ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,518 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a184277{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,518 INFO ui.ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,520 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@753fb1bc{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,520 INFO ui.ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,521 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c64f3e2{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,522 INFO ui.ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,523 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4ca1eac0{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,523 INFO ui.ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,524 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@b4abc44{/storage,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,525 INFO ui.ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,526 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a19ef8{/storage/json,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,526 INFO ui.ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,527 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f1c3f7c{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,528 INFO ui.ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,529 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@704d5d55{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,529 INFO ui.ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,531 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@858ee7{/environment,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,531 INFO ui.ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,532 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1bac67cf{/environment/json,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,533 INFO ui.ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,534 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6cd6d7d3{/executors,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,534 INFO ui.ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,535 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7991c35e{/executors/json,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,536 INFO ui.ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,540 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1db6f185{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,541 INFO ui.ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,543 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28256b2f{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,546 INFO ui.ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,578 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a1b962b{/static,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,581 INFO ui.ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,587 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ae680d6{/,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,590 INFO ui.ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,600 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e6a06ce{/api,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,600 INFO ui.ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,604 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@45eedd85{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,605 INFO ui.ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,607 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1cc42e2f{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,623 INFO ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 21:38:45,625 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@84ed9b6{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2023-04-26 21:38:45,626 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('test').master(\"yarn\") \\\n",
    ".config(\"spark.executor.instances\", 9) \\\n",
    ".config(\"spark.executor.memory\", \"1G\")  \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MUTE OUTPUT FROM SPARK\n",
    "logger = spark._jvm.org.apache.log4j\n",
    "logger.LogManager.getLogger(\"org\").setLevel(logger.Level.OFF)\n",
    "logger.LogManager.getLogger(\"akka\").setLevel(logger.Level.OFF)\n",
    "spark.conf.set(\"spark.driver.log.level\", \"OFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://namenode:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2f53c11fd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Event, White, Black, Result, WhiteElo, BlackElo, Opening, TimeControl, Termination, Moves,Eval, UTCTimestamp\n",
    "schema = StructType([ \\\n",
    "    StructField(\"Event\",StringType(),True), \\\n",
    "    StructField(\"White\",StringType(),True), \\\n",
    "    StructField(\"Black\",StringType(),True), \\\n",
    "    StructField(\"Result\", StringType(), True), \\\n",
    "    StructField(\"WhiteElo\", IntegerType(), True), \\\n",
    "    StructField(\"BlackElo\", IntegerType(), True), \\\n",
    "    StructField(\"Opening\",StringType(),True), \\\n",
    "    StructField(\"TimeControl\",StringType(),True), \\\n",
    "    StructField(\"Termination\",StringType(),True), \\\n",
    "    StructField(\"Moves\", StringType(), True), \\\n",
    "    StructField(\"Eval\", StringType(), True), \\\n",
    "    StructField(\"UTCTimestamp\", TimestampType(), True) \\\n",
    "  ])\n",
    "df = spark.read.csv(\"hdfs://namenode:9000/chess_2016_dataset/output/part*\", schema=schema).cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check Shape of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  (4871421, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape: \", (df.count(), len(df.columns)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert columns to appropriate types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Event', 'string'),\n",
       " ('White', 'string'),\n",
       " ('Black', 'string'),\n",
       " ('Result', 'string'),\n",
       " ('WhiteElo', 'int'),\n",
       " ('BlackElo', 'int'),\n",
       " ('Opening', 'string'),\n",
       " ('TimeControl', 'string'),\n",
       " ('Termination', 'string'),\n",
       " ('Moves', 'array<string>'),\n",
       " ('Eval', 'array<float>'),\n",
       " ('UTCTimestamp', 'timestamp')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = convert_types(df)\n",
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Eval Games and add blunders column for black and white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==========================================>              (24 + 4) / 32]\r"
     ]
    }
   ],
   "source": [
    "eval_difference = 3.0\n",
    "eval_games = df.where(col(\"Eval\")[0].isNotNull())\n",
    "eval_games = eval_games.withColumn(\"WhiteBlunders\", (find_white_blunders(col(\"Eval\"), lit(eval_difference)))).cache()\n",
    "eval_games = eval_games.withColumn(\"BlackBlunders\", (find_black_blunders(col(\"Eval\"), lit(eval_difference))))\n",
    "eval_games.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_games.select(\"TimeControl\", \"White\", \"WhiteElo\", \"WhiteBlunders\", \"Black\", \"BlackElo\", \"BlackBlunders\", \"Result\", \"Termination\") \\\n",
    "    .orderBy(col(\"WhiteBlunders\").desc(), col(\"BlackBlunders\").desc()).limit(10).toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"TimeControl\", \"White\", \"WhiteElo\", \"Black\", \"BlackElo\", \"Result\", \"Termination\") \\\n",
    "    .orderBy(col(\"White\").desc()).limit(10).toPandas().head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot Most Blundered Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eval_game_optimized(eval_games=eval_games)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# plot_eval_game(eval_games=eval_games) # garbo shitballz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By\n",
    "- Time control ~ (60, 120, 180, 600) etc...\n",
    "- Elo-Brackets ~ ([1200, 1400], [1500, 1700], [2000-2200]) etc..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Control Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_control_blunders_averages = eval_games \\\n",
    "    .groupBy(\"TimeControl\") \\\n",
    "    .agg(mean(\"WhiteBlunders\").alias(\"avg(WhiteBlunders)\"), \\\n",
    "        mean(\"BlackBlunders\").alias(\"avg(BlackBlunders)\"), \\\n",
    "        mean(\"BlackElo\").alias(\"avg(BlackElo)\"), \\\n",
    "        mean(\"WhiteElo\").alias(\"avg(WhiteElo)\"), \\\n",
    "        count(\"TimeControl\").alias(\"count(TimeControl)\")) \\\n",
    "    .withColumn(\"avg(WhiteBlunders)\", format_number(\"avg(WhiteBlunders)\", 1)) \\\n",
    "    .withColumn(\"avg(BlackBlunders)\", format_number(\"avg(BlackBlunders)\", 1)) \\\n",
    "    .withColumn(\"avg(WhiteElo)\", format_number(\"avg(WhiteElo)\", 0)) \\\n",
    "    .withColumn(\"avg(BlackElo)\", format_number(\"avg(BlackElo)\", 0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_control_blunders_averages \\\n",
    "    .orderBy(col(\"avg(WhiteBlunders)\").desc(), col(\"avg(BlackBlunders)\").desc()) \\\n",
    "    .where(col(\"count(TimeControl)\")>10000) \\\n",
    "    .limit(10) \\\n",
    "    .toPandas() \\\n",
    "    .head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elo Brackets Grouping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start off by Creating a new spark daraframe column called \"EloBracket\" which we will later use to group and aggregrate by. When grouping the players by elo brackets we want to use a range that makes sense such that there are not 1 bracket that contains 80% of the playerbase and ones that only contain a small fraction. E.g We want evenly distributed amount of players in each bracket (as far as that is possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by getting all the elo column values in the dataframe.\n",
    "elo_list = eval_games.select(collect_list(\"WhiteElo\")).first()[0]\n",
    "sns.set_theme(style=\"ticks\")\n",
    "sns.set_style('darkgrid')\n",
    "sns.distplot(elo_list, kde=True, color ='green', bins=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"EloBracket\" column should be of type String and contain values in format: \"0-1200\", \"1200-1600\", \"1600-2000\", \"2000-3000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_games = eval_games.withColumn(\"EloBracket\", \\\n",
    "                         when((0 < eval_games.WhiteElo) & (eval_games.WhiteElo < 1500), lit(\"<1500\")) \\\n",
    "                        .when((1500 <= eval_games.WhiteElo) & (eval_games.WhiteElo <= 1750), lit(\"1500-1750\")) \\\n",
    "                        .when((1750 < eval_games.WhiteElo) & (eval_games.WhiteElo <= 2000), lit(\"1751-2000\")) \\\n",
    "                        .otherwise(lit(\">2000\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_games.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELO and Blunders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_bracket_white_blunders_averages = eval_games \\\n",
    "    .groupBy(\"EloBracket\") \\\n",
    "    .agg(mean(\"WhiteBlunders\"), count(\"EloBracket\")) \\\n",
    "    .withColumn(\"avg(WhiteBlunders)\", format_number(\"avg(WhiteBlunders)\", 1))\n",
    "\n",
    "elo_bracket_black_blunders_averages = eval_games \\\n",
    "    .groupBy(\"EloBracket\") \\\n",
    "    .agg(mean(\"BlackBlunders\"), count(\"EloBracket\")) \\\n",
    "    .withColumn(\"avg(BlackBlunders)\", format_number(\"avg(BlackBlunders)\", 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_bracket_white_blunders_averages.orderBy(col(\"avg(WhiteBlunders)\").desc()).limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_bracket_black_blunders_averages.orderBy(col(\"avg(BlackBlunders)\").desc()).limit(10).toPandas().head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining two tables to test optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_eval = df.join(eval_games, \\\n",
    "                [df.White == eval_games.White, \\\n",
    "                df.UTCTimestamp == eval_games.UTCTimestamp ]\\\n",
    "                ,\"outer\" ) \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_eval.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With hash-broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enable broadcast Join and \n",
    "#Set Threshold limit of size in bytes of a DataFrame to broadcast\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 104857600)\n",
    "\n",
    "#Disable broadcast Join\n",
    "# spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform broadast join\n",
    "from pyspark.sql.functions import broadcast, col\n",
    "\n",
    "games_eval= df.join(\n",
    "  broadcast(eval_games),\n",
    "  [\"White\", \"UTCTimestamp\"],\n",
    "  \"outer\"\n",
    ").show()\n",
    "\n",
    "  # eval_games(\"code\")==df(\"UNIT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# games_eval \\\n",
    "#     .orderBy(col(\"count(TimeControl)\").desc()) \\\n",
    "#     .limit(10) \\\n",
    "#     .toPandas() \\\n",
    "#     .head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
