{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries #\n",
    "import findspark\n",
    "# findspark.find()\n",
    "findspark.init()\n",
    "from pyspark.sql.functions import split, col, array_contains, translate, round, size, when, udf, lit, mean, count, format_number, collect_list, expr\n",
    "from pyspark.sql.types import TimestampType, MapType, IntegerType, StringType, ArrayType, FloatType, StructField, StructType\n",
    "from pyspark.sql import SparkSession\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('test').master(\"yarn\") \\\n",
    ".config(\"spark.executor.instances\", 12) \\\n",
    ".config(\"spark.executor.memory\", \"1G\")  \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MUTE OUTPUT FROM SPARK\n",
    "## Enable during development\n",
    "logger = spark._jvm.org.apache.log4j\n",
    "logger.LogManager.getLogger(\"org\").setLevel(logger.Level.OFF)\n",
    "logger.LogManager.getLogger(\"akka\").setLevel(logger.Level.OFF)\n",
    "spark.conf.set(\"spark.driver.log.level\", \"OFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Event, White, Black, Result, WhiteElo, BlackElo, Opening, TimeControl, Termination, Moves,Eval, UTCTimestamp\n",
    "schema = StructType([ \\\n",
    "    StructField(\"Event\",StringType(),True), \\\n",
    "    StructField(\"White\",StringType(),True), \\\n",
    "    StructField(\"Black\",StringType(),True), \\\n",
    "    StructField(\"Result\", StringType(), True), \\\n",
    "    StructField(\"WhiteElo\", IntegerType(), True), \\\n",
    "    StructField(\"BlackElo\", IntegerType(), True), \\\n",
    "    StructField(\"Opening\",StringType(),True), \\\n",
    "    StructField(\"TimeControl\",StringType(),True), \\\n",
    "    StructField(\"Termination\",StringType(),True), \\\n",
    "    StructField(\"Moves\", StringType(), True), \\\n",
    "    StructField(\"Eval\", StringType(), True), \\\n",
    "    StructField(\"UTCTimestamp\", TimestampType(), True) \\\n",
    "  ])\n",
    "df = spark.read.csv(\"hdfs://namenode:9000/chess_2016_dataset/output/part*\", schema=schema).cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check Shape of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"shape: \", (df.count(), len(df.columns)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert columns to appropriate types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convert_types(df)\n",
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Eval Games and add blunders column for black and white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_difference = 3.0\n",
    "eval_games = df.where(col(\"Eval\")[0].isNotNull())\n",
    "eval_games = eval_games.withColumn(\"WhiteBlunders\", (find_white_blunders(col(\"Eval\"), lit(eval_difference)))).cache()\n",
    "eval_games = eval_games.withColumn(\"BlackBlunders\", (find_black_blunders(col(\"Eval\"), lit(eval_difference))))\n",
    "eval_games.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_games.select(\"TimeControl\", \"White\", \"WhiteElo\", \"WhiteBlunders\", \"Black\", \"BlackElo\", \"BlackBlunders\", \"Result\", \"Termination\") \\\n",
    "    .orderBy(col(\"WhiteBlunders\").desc(), col(\"BlackBlunders\").desc()).limit(10).toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"TimeControl\", \"White\", \"WhiteElo\", \"Black\", \"BlackElo\", \"Result\", \"Termination\") \\\n",
    "    .orderBy(col(\"White\").desc()).limit(10).toPandas().head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot Most Blundered Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eval_game_optimized(eval_games=eval_games)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By\n",
    "- Time control ~ (60, 120, 180, 600) etc...\n",
    "- Elo-Brackets ~ ([1200, 1400], [1500, 1700], [2000-2200]) etc..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Control Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_control_blunders_averages = eval_games \\\n",
    "    .groupBy(\"TimeControl\") \\\n",
    "    .agg(mean(\"WhiteBlunders\").alias(\"avg(WhiteBlunders)\"), \\\n",
    "        mean(\"BlackBlunders\").alias(\"avg(BlackBlunders)\"), \\\n",
    "        mean(\"BlackElo\").alias(\"avg(BlackElo)\"), \\\n",
    "        mean(\"WhiteElo\").alias(\"avg(WhiteElo)\"), \\\n",
    "        count(\"TimeControl\").alias(\"count(TimeControl)\")) \\\n",
    "    .withColumn(\"avg(WhiteBlunders)\", format_number(\"avg(WhiteBlunders)\", 1)) \\\n",
    "    .withColumn(\"avg(BlackBlunders)\", format_number(\"avg(BlackBlunders)\", 1)) \\\n",
    "    .withColumn(\"avg(WhiteElo)\", format_number(\"avg(WhiteElo)\", 0)) \\\n",
    "    .withColumn(\"avg(BlackElo)\", format_number(\"avg(BlackElo)\", 0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_control_blunders_averages \\\n",
    "    .orderBy(col(\"avg(WhiteBlunders)\").desc(), col(\"avg(BlackBlunders)\").desc()) \\\n",
    "    .where(col(\"count(TimeControl)\")>10000) \\\n",
    "    .limit(10) \\\n",
    "    .toPandas() \\\n",
    "    .head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elo Brackets Grouping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start off by Creating a new spark daraframe column called \"EloBracket\" which we will later use to group and aggregrate by. When grouping the players by elo brackets we want to use a range that makes sense such that there are not 1 bracket that contains 80% of the playerbase and ones that only contain a small fraction. E.g We want evenly distributed amount of players in each bracket (as far as that is possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by getting all the elo column values in the dataframe.\n",
    "elo_list = eval_games.select(collect_list(\"WhiteElo\")).first()[0]\n",
    "sns.set_theme(style=\"ticks\")\n",
    "sns.set_style('darkgrid')\n",
    "sns.distplot(elo_list, kde=True, color ='green', bins=20)\n",
    "plt.savefig(\"chess_2016_dataset/images/elo-distribution.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"EloBracket\" column should be of type String and contain values in format: \"0-1200\", \"1200-1600\", \"1600-2000\", \"2000-3000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_games = eval_games.withColumn(\"EloBracket\", \\\n",
    "                         when((0 < eval_games.WhiteElo) & (eval_games.WhiteElo < 1500), lit(\"<1500\")) \\\n",
    "                        .when((1500 <= eval_games.WhiteElo) & (eval_games.WhiteElo <= 1750), lit(\"1500-1750\")) \\\n",
    "                        .when((1750 < eval_games.WhiteElo) & (eval_games.WhiteElo <= 2000), lit(\"1751-2000\")) \\\n",
    "                        .otherwise(lit(\">2000\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_games.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELO and Blunders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_bracket_white_blunders_averages = eval_games \\\n",
    "    .groupBy(\"EloBracket\") \\\n",
    "    .agg(mean(\"WhiteBlunders\"), count(\"EloBracket\")) \\\n",
    "    .withColumn(\"avg(WhiteBlunders)\", format_number(\"avg(WhiteBlunders)\", 1))\n",
    "\n",
    "elo_bracket_black_blunders_averages = eval_games \\\n",
    "    .groupBy(\"EloBracket\") \\\n",
    "    .agg(mean(\"BlackBlunders\"), count(\"EloBracket\")) \\\n",
    "    .withColumn(\"avg(BlackBlunders)\", format_number(\"avg(BlackBlunders)\", 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_bracket_white_blunders_averages.orderBy(col(\"avg(WhiteBlunders)\").desc()).limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_bracket_black_blunders_averages.orderBy(col(\"avg(BlackBlunders)\").desc()).limit(10).toPandas().head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining two tables to test optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_eval = df.join(eval_games, \\\n",
    "                [df.White == eval_games.White, \\\n",
    "                df.UTCTimestamp == eval_games.UTCTimestamp ]\\\n",
    "                ,\"outer\" ) \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_eval.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With hash-broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enable broadcast Join and \n",
    "#Set Threshold limit of size in bytes of a DataFrame to broadcast\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 104857600)\n",
    "\n",
    "#Disable broadcast Join\n",
    "# spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform broadast join\n",
    "from pyspark.sql.functions import broadcast, col\n",
    "\n",
    "games_eval= df.join(\n",
    "  broadcast(eval_games),\n",
    "  [\"White\", \"UTCTimestamp\"],\n",
    "  \"outer\"\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
