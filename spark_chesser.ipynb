{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2023-03-30 12:51:32,534 INFO spark.SparkContext: Running Spark version 3.3.2\n",
      "2023-03-30 12:51:33,248 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-03-30 12:51:33,251 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2023-03-30 12:51:33,251 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-03-30 12:51:33,252 INFO spark.SparkContext: Submitted application: test\n",
      "2023-03-30 12:51:33,307 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2023-03-30 12:51:33,346 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "2023-03-30 12:51:33,354 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2023-03-30 12:51:33,458 INFO spark.SecurityManager: Changing view acls to: ubuntu\n",
      "2023-03-30 12:51:33,459 INFO spark.SecurityManager: Changing modify acls to: ubuntu\n",
      "2023-03-30 12:51:33,460 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-03-30 12:51:33,460 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-03-30 12:51:33,461 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()\n",
      "2023-03-30 12:51:34,208 INFO util.Utils: Successfully started service 'sparkDriver' on port 42001.\n",
      "2023-03-30 12:51:34,359 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2023-03-30 12:51:34,492 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2023-03-30 12:51:34,607 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2023-03-30 12:51:34,610 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2023-03-30 12:51:34,786 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2023-03-30 12:51:34,857 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-311e5cae-180c-4f44-a005-d0d0990b10e1\n",
      "2023-03-30 12:51:34,923 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "2023-03-30 12:51:34,986 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2023-03-30 12:51:35,157 INFO util.log: Logging initialized @12326ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2023-03-30 12:51:35,427 INFO server.Server: jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09\n",
      "2023-03-30 12:51:35,505 INFO server.Server: Started @12675ms\n",
      "2023-03-30 12:51:35,598 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2023-03-30 12:51:35,619 INFO server.AbstractConnector: Started ServerConnector@d059dfa{HTTP/1.1, (http/1.1)}{0.0.0.0:4041}\n",
      "2023-03-30 12:51:35,619 INFO util.Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "2023-03-30 12:51:35,666 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3d461e68{/,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:37,397 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at namenode/192.168.11.141:8032\n",
      "2023-03-30 12:51:40,138 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-03-30 12:51:40,140 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-03-30 12:51:40,178 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)\n",
      "2023-03-30 12:51:40,179 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "2023-03-30 12:51:40,180 INFO yarn.Client: Setting up container launch context for our AM\n",
      "2023-03-30 12:51:40,194 INFO yarn.Client: Setting up the launch environment for our AM container\n",
      "2023-03-30 12:51:40,205 INFO yarn.Client: Preparing resources for our AM container\n",
      "2023-03-30 12:51:40,291 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "2023-03-30 12:51:47,544 INFO yarn.Client: Uploading resource file:/tmp/spark-065e2dfd-eb8c-45f5-b89a-16aa5c758125/__spark_libs__6715638799461762087.zip -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1678881332295_0026/__spark_libs__6715638799461762087.zip\n",
      "2023-03-30 12:51:49,605 INFO yarn.Client: Uploading resource file:/home/ubuntu/spark/python/lib/pyspark.zip -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1678881332295_0026/pyspark.zip\n",
      "2023-03-30 12:51:49,714 INFO yarn.Client: Uploading resource file:/home/ubuntu/spark/python/lib/py4j-0.10.9.5-src.zip -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1678881332295_0026/py4j-0.10.9.5-src.zip\n",
      "2023-03-30 12:51:50,675 INFO yarn.Client: Uploading resource file:/tmp/spark-065e2dfd-eb8c-45f5-b89a-16aa5c758125/__spark_conf__4868750497546233289.zip -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1678881332295_0026/__spark_conf__.zip\n",
      "2023-03-30 12:51:50,798 INFO spark.SecurityManager: Changing view acls to: ubuntu\n",
      "2023-03-30 12:51:50,798 INFO spark.SecurityManager: Changing modify acls to: ubuntu\n",
      "2023-03-30 12:51:50,798 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-03-30 12:51:50,798 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-03-30 12:51:50,798 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()\n",
      "2023-03-30 12:51:50,997 INFO yarn.Client: Submitting application application_1678881332295_0026 to ResourceManager\n",
      "2023-03-30 12:51:51,127 INFO impl.YarnClientImpl: Submitted application application_1678881332295_0026\n",
      "2023-03-30 12:51:52,133 INFO yarn.Client: Application report for application_1678881332295_0026 (state: ACCEPTED)\n",
      "2023-03-30 12:51:52,145 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1680180711055\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://namenode:8088/proxy/application_1678881332295_0026/\n",
      "\t user: ubuntu\n",
      "2023-03-30 12:51:53,150 INFO yarn.Client: Application report for application_1678881332295_0026 (state: ACCEPTED)\n",
      "2023-03-30 12:51:54,154 INFO yarn.Client: Application report for application_1678881332295_0026 (state: ACCEPTED)\n",
      "2023-03-30 12:51:55,157 INFO yarn.Client: Application report for application_1678881332295_0026 (state: ACCEPTED)\n",
      "2023-03-30 12:51:56,160 INFO yarn.Client: Application report for application_1678881332295_0026 (state: ACCEPTED)\n",
      "2023-03-30 12:51:57,162 INFO yarn.Client: Application report for application_1678881332295_0026 (state: ACCEPTED)\n",
      "2023-03-30 12:51:58,165 INFO yarn.Client: Application report for application_1678881332295_0026 (state: ACCEPTED)\n",
      "2023-03-30 12:51:58,686 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> namenode, PROXY_URI_BASES -> http://namenode:8088/proxy/application_1678881332295_0026), /proxy/application_1678881332295_0026\n",
      "2023-03-30 12:51:59,169 INFO yarn.Client: Application report for application_1678881332295_0026 (state: RUNNING)\n",
      "2023-03-30 12:51:59,169 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 192.168.11.39\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1680180711055\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://namenode:8088/proxy/application_1678881332295_0026/\n",
      "\t user: ubuntu\n",
      "2023-03-30 12:51:59,172 INFO cluster.YarnClientSchedulerBackend: Application application_1678881332295_0026 has started running.\n",
      "2023-03-30 12:51:59,186 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39703.\n",
      "2023-03-30 12:51:59,186 INFO netty.NettyBlockTransferService: Server created on namenode:39703\n",
      "2023-03-30 12:51:59,188 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2023-03-30 12:51:59,200 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, namenode, 39703, None)\n",
      "2023-03-30 12:51:59,209 INFO storage.BlockManagerMasterEndpoint: Registering block manager namenode:39703 with 366.3 MiB RAM, BlockManagerId(driver, namenode, 39703, None)\n",
      "2023-03-30 12:51:59,215 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, namenode, 39703, None)\n",
      "2023-03-30 12:51:59,217 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, namenode, 39703, None)\n",
      "2023-03-30 12:51:59,549 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@3d461e68{/,null,STOPPED,@Spark}\n",
      "2023-03-30 12:51:59,551 INFO ui.ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,561 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b9c4a00{/jobs,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,562 INFO ui.ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,564 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@198dacbc{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,565 INFO ui.ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,570 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@bf835e7{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,572 INFO ui.ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,573 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c49d10b{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,574 INFO ui.ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,578 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@55cfdaf{/stages,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,578 INFO ui.ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,579 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5bc80b82{/stages/json,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,579 INFO ui.ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,581 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f6bd879{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,582 INFO ui.ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,583 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@78661ca2{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,583 INFO ui.ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,584 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a027a69{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,584 INFO ui.ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,585 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30ef4bb6{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,586 INFO ui.ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,590 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c5d44be{/storage,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,590 INFO ui.ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,594 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30dc8590{/storage/json,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,595 INFO ui.ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,596 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b4e0d2{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,597 INFO ui.ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,598 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28c31c12{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,599 INFO ui.ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,600 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4ef336f2{/environment,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,601 INFO ui.ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,602 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b3a6e24{/environment/json,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,603 INFO ui.ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,604 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@169b6c8{/executors,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,605 INFO ui.ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,606 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@c11597e{/executors/json,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,607 INFO ui.ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,610 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d661405{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,611 INFO ui.ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,612 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@f1e904{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,613 INFO ui.ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,664 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@151bfdb1{/static,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,665 INFO ui.ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,667 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@75269d37{/,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,668 INFO ui.ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,671 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7bcd4d77{/api,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,672 INFO ui.ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,674 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51f3aa1a{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,674 INFO ui.ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,676 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36843898{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:51:59,683 INFO ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:51:59,691 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36e6b58b{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:52:00,376 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "2023-03-30 12:52:04,985 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.11.39:38512) with ID 2,  ResourceProfileId 0\n",
      "2023-03-30 12:52:05,150 INFO storage.BlockManagerMasterEndpoint: Registering block manager datanode3:42711 with 366.3 MiB RAM, BlockManagerId(2, datanode3, 42711, None)\n",
      "2023-03-30 12:52:06,907 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\n",
      "2023-03-30 12:52:07,358 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "2023-03-30 12:52:07,364 INFO internal.SharedState: Warehouse path is 'file:/home/ubuntu/chess_project/spark-warehouse'.\n",
      "2023-03-30 12:52:07,386 INFO ui.ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:52:07,389 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53d77c51{/SQL,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:52:07,389 INFO ui.ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:52:07,390 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c4d07a6{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:52:07,390 INFO ui.ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:52:07,392 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@38ab8e91{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:52:07,392 INFO ui.ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:52:07,393 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@728e7719{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:52:07,394 INFO ui.ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-30 12:52:07,395 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7b39f6d{/static/sql,null,AVAILABLE,@Spark}\n",
      "2023-03-30 12:52:07,910 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.11.172:41942) with ID 1,  ResourceProfileId 0\n",
      "2023-03-30 12:52:08,059 INFO storage.BlockManagerMasterEndpoint: Registering block manager datanode1:35681 with 366.3 MiB RAM, BlockManagerId(1, datanode1, 35681, None)\n",
      "2023-03-30 12:52:09,031 INFO datasources.InMemoryFileIndex: It took 114 ms to list leaf files for 1 paths.\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries #\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql.functions import split, col, array_contains, translate, round, size, when, udf, lit, mean, count, format_number\n",
    "from pyspark.sql.types import TimestampType, MapType, IntegerType, StringType, ArrayType, FloatType, StructField, StructType\n",
    "from pyspark.sql import SparkSession\n",
    "from helper import *\n",
    "import math\n",
    "\n",
    "spark = SparkSession.builder.appName('test').master(\"yarn\").getOrCreate()\n",
    "#Event, White, Black, Result, WhiteElo, BlackElo, Opening, TimeControl, Termination, Moves,Eval, UTCTimestamp\n",
    "schema = StructType([ \\\n",
    "    StructField(\"Event\",StringType(),True), \\\n",
    "    StructField(\"White\",StringType(),True), \\\n",
    "    StructField(\"Black\",StringType(),True), \\\n",
    "    StructField(\"Result\", StringType(), True), \\\n",
    "    StructField(\"WhiteElo\", IntegerType(), True), \\\n",
    "    StructField(\"BlackElo\", IntegerType(), True), \\\n",
    "    StructField(\"Opening\",StringType(),True), \\\n",
    "    StructField(\"TimeControl\",StringType(),True), \\\n",
    "    StructField(\"Termination\",StringType(),True), \\\n",
    "    StructField(\"Moves\", StringType(), True), \\\n",
    "    StructField(\"Eval\", StringType(), True), \\\n",
    "    StructField(\"UTCTimestamp\", TimestampType(), True) \\\n",
    "  ])\n",
    "df = spark.read.csv(\"hdfs://namenode:9000/chess_2016_dataset/output/part*\", schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9db9416a30>"
      ],
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://namenode:4041\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>test</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check Shape of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2023-03-30 12:52:13,150 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-03-30 12:52:13,153 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-03-30 12:52:13,156 INFO datasources.FileSourceStrategy: Output Data Schema: struct<>\n",
      "2023-03-30 12:52:14,295 INFO codegen.CodeGenerator: Code generated in 332.648369 ms\n",
      "2023-03-30 12:52:14,387 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 485.1 KiB, free 365.8 MiB)\n",
      "2023-03-30 12:52:14,541 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 53.7 KiB, free 365.8 MiB)\n",
      "2023-03-30 12:52:14,545 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on namenode:39703 (size: 53.7 KiB, free: 366.2 MiB)\n",
      "2023-03-30 12:52:14,561 INFO spark.SparkContext: Created broadcast 0 from count at NativeMethodAccessorImpl.java:0\n",
      "2023-03-30 12:52:14,615 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-03-30 12:52:14,954 INFO scheduler.DAGScheduler: Registering RDD 3 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "2023-03-30 12:52:14,962 INFO scheduler.DAGScheduler: Got map stage job 0 (count at NativeMethodAccessorImpl.java:0) with 16 output partitions\n",
      "2023-03-30 12:52:14,962 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0)\n",
      "2023-03-30 12:52:14,963 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-03-30 12:52:14,965 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-03-30 12:52:14,984 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-03-30 12:52:15,334 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.6 KiB, free 365.8 MiB)\n",
      "2023-03-30 12:52:15,343 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 365.7 MiB)\n",
      "2023-03-30 12:52:15,344 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on namenode:39703 (size: 8.4 KiB, free: 366.2 MiB)\n",
      "2023-03-30 12:52:15,348 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
      "2023-03-30 12:52:15,374 INFO scheduler.DAGScheduler: Submitting 16 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "2023-03-30 12:52:15,375 INFO cluster.YarnScheduler: Adding task set 0.0 with 16 tasks resource profile 0\n",
      "2023-03-30 12:52:15,438 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (datanode3, executor 2, partition 0, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:15,454 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 1) (datanode1, executor 1, partition 2, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:15,861 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on datanode3:42711 (size: 8.4 KiB, free: 366.3 MiB)\n",
      "2023-03-30 12:52:15,865 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on datanode1:35681 (size: 8.4 KiB, free: 366.3 MiB)\n",
      "2023-03-30 12:52:17,020 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on datanode3:42711 (size: 53.7 KiB, free: 366.2 MiB)\n",
      "2023-03-30 12:52:17,135 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on datanode1:35681 (size: 53.7 KiB, free: 366.2 MiB)\n",
      "2023-03-30 12:52:19,636 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 2) (datanode3, executor 2, partition 1, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:19,653 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4227 ms on datanode3 (executor 2) (1/16)\n",
      "2023-03-30 12:52:19,712 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (datanode1, executor 1, partition 3, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:19,716 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 1) in 4265 ms on datanode1 (executor 1) (2/16)\n",
      "2023-03-30 12:52:20,528 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 0.0 (TID 4) (datanode3, executor 2, partition 6, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:20,531 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 2) in 896 ms on datanode3 (executor 2) (3/16)\n",
      "2023-03-30 12:52:20,679 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 0.0 (TID 5) (datanode1, executor 1, partition 4, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:20,684 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 973 ms on datanode1 (executor 1) (4/16)\n",
      "2023-03-30 12:52:21,540 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 0.0 (TID 6) (datanode1, executor 1, partition 5, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:21,542 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 0.0 (TID 5) in 864 ms on datanode1 (executor 1) (5/16)\n",
      "2023-03-30 12:52:21,567 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 0.0 (TID 7) (datanode3, executor 2, partition 9, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:21,568 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 0.0 (TID 4) in 1041 ms on datanode3 (executor 2) (6/16)\n",
      "2023-03-30 12:52:22,455 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 0.0 (TID 8) (datanode3, executor 2, partition 12, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:22,458 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 0.0 (TID 7) in 892 ms on datanode3 (executor 2) (7/16)\n",
      "2023-03-30 12:52:22,664 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 0.0 (TID 9) (datanode1, executor 1, partition 7, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:22,666 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 0.0 (TID 6) in 1126 ms on datanode1 (executor 1) (8/16)\n",
      "2023-03-30 12:52:23,312 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 0.0 (TID 10) (datanode3, executor 2, partition 13, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:23,313 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 0.0 (TID 8) in 859 ms on datanode3 (executor 2) (9/16)\n",
      "2023-03-30 12:52:23,752 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 0.0 (TID 11) (datanode1, executor 1, partition 8, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:23,755 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 0.0 (TID 9) in 1092 ms on datanode1 (executor 1) (10/16)\n",
      "2023-03-30 12:52:24,112 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 0.0 (TID 10) in 800 ms on datanode3 (executor 2) (11/16)\n",
      "2023-03-30 12:52:24,549 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 0.0 (TID 12) (datanode1, executor 1, partition 10, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:24,551 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 0.0 (TID 11) in 799 ms on datanode1 (executor 1) (12/16)\n",
      "2023-03-30 12:52:25,570 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 0.0 (TID 13) (datanode1, executor 1, partition 11, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:25,573 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 0.0 (TID 12) in 1025 ms on datanode1 (executor 1) (13/16)\n",
      "2023-03-30 12:52:26,351 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (datanode1, executor 1, partition 14, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:26,353 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 0.0 (TID 13) in 784 ms on datanode1 (executor 1) (14/16)\n",
      "2023-03-30 12:52:26,884 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (datanode3, executor 2, partition 15, RACK_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:27,143 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 795 ms on datanode1 (executor 1) (15/16)\n",
      "2023-03-30 12:52:27,645 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 769 ms on datanode3 (executor 2) (16/16)\n",
      "2023-03-30 12:52:27,650 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 12.472 s\n",
      "2023-03-30 12:52:27,648 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2023-03-30 12:52:27,653 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-03-30 12:52:27,656 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-03-30 12:52:27,656 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-03-30 12:52:27,657 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-03-30 12:52:27,754 INFO codegen.CodeGenerator: Code generated in 32.815396 ms\n",
      "2023-03-30 12:52:27,815 INFO spark.SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "2023-03-30 12:52:27,821 INFO scheduler.DAGScheduler: Got job 1 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-03-30 12:52:27,822 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (count at NativeMethodAccessorImpl.java:0)\n",
      "2023-03-30 12:52:27,823 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
      "2023-03-30 12:52:27,824 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-03-30 12:52:27,834 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-03-30 12:52:27,856 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 11.1 KiB, free 365.7 MiB)\n",
      "2023-03-30 12:52:27,860 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 365.7 MiB)\n",
      "2023-03-30 12:52:27,861 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on namenode:39703 (size: 5.5 KiB, free: 366.2 MiB)\n",
      "2023-03-30 12:52:27,867 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1513\n",
      "2023-03-30 12:52:27,869 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-03-30 12:52:27,869 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "2023-03-30 12:52:27,873 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 16) (datanode1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:27,911 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on datanode1:35681 (size: 5.5 KiB, free: 366.2 MiB)\n",
      "2023-03-30 12:52:28,057 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.11.172:41942\n",
      "2023-03-30 12:52:28,220 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 16) in 349 ms on datanode1 (executor 1) (1/1)\n",
      "2023-03-30 12:52:28,221 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "2023-03-30 12:52:28,223 INFO scheduler.DAGScheduler: ResultStage 2 (count at NativeMethodAccessorImpl.java:0) finished in 0.371 s\n",
      "2023-03-30 12:52:28,228 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-03-30 12:52:28,228 INFO cluster.YarnScheduler: Killing all running tasks in stage 2: Stage finished\n",
      "2023-03-30 12:52:28,231 INFO scheduler.DAGScheduler: Job 1 finished: count at NativeMethodAccessorImpl.java:0, took 0.414672 s\n",
      "shape:  (3113065, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape: \", (df.count(), len(df.columns)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert columns to appropriate types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Event', 'string'),\n",
       " ('White', 'string'),\n",
       " ('Black', 'string'),\n",
       " ('Result', 'string'),\n",
       " ('WhiteElo', 'int'),\n",
       " ('BlackElo', 'int'),\n",
       " ('Opening', 'string'),\n",
       " ('TimeControl', 'string'),\n",
       " ('Termination', 'string'),\n",
       " ('Moves', 'array<string>'),\n",
       " ('Eval', 'array<float>'),\n",
       " ('UTCTimestamp', 'timestamp')]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df = convert_types(df)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2023-03-30 12:52:33,872 INFO datasources.FileSourceStrategy: Pushed Filters: IsNotNull(Eval)\n",
      "2023-03-30 12:52:33,873 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(Eval#10),isnotnull(cast(split(translate(Eval#10, ', ), ,, -1) as array<float>)[0])\n",
      "2023-03-30 12:52:33,875 INFO datasources.FileSourceStrategy: Output Data Schema: struct<White: string, Black: string, Result: string, WhiteElo: int, BlackElo: int ... 6 more fields>\n",
      "2023-03-30 12:52:33,975 INFO codegen.CodeGenerator: Code generated in 44.71062 ms\n",
      "2023-03-30 12:52:34,035 INFO codegen.CodeGenerator: Code generated in 40.750894 ms\n",
      "2023-03-30 12:52:34,225 INFO codegen.CodeGenerator: Code generated in 140.514741 ms\n",
      "2023-03-30 12:52:34,250 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 485.1 KiB, free 365.3 MiB)\n",
      "2023-03-30 12:52:34,282 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 53.7 KiB, free 365.2 MiB)\n",
      "2023-03-30 12:52:34,283 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on namenode:39703 (size: 53.7 KiB, free: 366.2 MiB)\n",
      "2023-03-30 12:52:34,284 INFO spark.SparkContext: Created broadcast 3 from toPandas at /tmp/ipykernel_182074/985589280.py:5\n",
      "2023-03-30 12:52:34,288 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-03-30 12:52:34,459 INFO spark.SparkContext: Starting job: toPandas at /tmp/ipykernel_182074/985589280.py:5\n",
      "2023-03-30 12:52:34,461 INFO scheduler.DAGScheduler: Got job 2 (toPandas at /tmp/ipykernel_182074/985589280.py:5) with 16 output partitions\n",
      "2023-03-30 12:52:34,461 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (toPandas at /tmp/ipykernel_182074/985589280.py:5)\n",
      "2023-03-30 12:52:34,461 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-03-30 12:52:34,462 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-03-30 12:52:34,464 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at toPandas at /tmp/ipykernel_182074/985589280.py:5), which has no missing parents\n",
      "2023-03-30 12:52:34,498 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 45.2 KiB, free 365.2 MiB)\n",
      "2023-03-30 12:52:34,506 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 365.1 MiB)\n",
      "2023-03-30 12:52:34,508 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on namenode:39703 (size: 20.1 KiB, free: 366.2 MiB)\n",
      "2023-03-30 12:52:34,509 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\n",
      "2023-03-30 12:52:34,513 INFO scheduler.DAGScheduler: Submitting 16 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at toPandas at /tmp/ipykernel_182074/985589280.py:5) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "2023-03-30 12:52:34,513 INFO cluster.YarnScheduler: Adding task set 3.0 with 16 tasks resource profile 0\n",
      "2023-03-30 12:52:34,517 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 17) (datanode3, executor 2, partition 0, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:34,517 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 3.0 (TID 18) (datanode1, executor 1, partition 2, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:34,547 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on datanode1:35681 (size: 20.1 KiB, free: 366.2 MiB)\n",
      "2023-03-30 12:52:34,548 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on datanode3:42711 (size: 20.1 KiB, free: 366.2 MiB)\n",
      "2023-03-30 12:52:35,897 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on datanode3:42711 (size: 53.7 KiB, free: 366.2 MiB)\n",
      "2023-03-30 12:52:36,628 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on datanode1:35681 (size: 53.7 KiB, free: 366.2 MiB)\n",
      "2023-03-30 12:52:41,772 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 19) (datanode3, executor 2, partition 1, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:41,838 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 17) in 7322 ms on datanode3 (executor 2) (1/16)\n",
      "2023-03-30 12:52:41,884 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 3.0 (TID 20) (datanode1, executor 1, partition 3, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:41,892 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 48739\n",
      "2023-03-30 12:52:41,893 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 3.0 (TID 18) in 7376 ms on datanode1 (executor 1) (2/16)\n",
      "2023-03-30 12:52:46,692 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 3.0 (TID 21) (datanode3, executor 2, partition 6, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:46,700 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 19) in 4929 ms on datanode3 (executor 2) (3/16)\n",
      "2023-03-30 12:52:46,967 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 3.0 (TID 22) (datanode1, executor 1, partition 4, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:46,978 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 3.0 (TID 20) in 5095 ms on datanode1 (executor 1) (4/16)\n",
      "2023-03-30 12:52:51,830 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 3.0 (TID 23) (datanode1, executor 1, partition 5, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:51,839 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 3.0 (TID 22) in 4872 ms on datanode1 (executor 1) (5/16)\n",
      "2023-03-30 12:52:52,016 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 3.0 (TID 24) (datanode3, executor 2, partition 9, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:52,023 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 3.0 (TID 21) in 5331 ms on datanode3 (executor 2) (6/16)\n",
      "2023-03-30 12:52:56,470 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 3.0 (TID 25) (datanode1, executor 1, partition 7, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:56,478 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 3.0 (TID 23) in 4649 ms on datanode1 (executor 1) (7/16)\n",
      "2023-03-30 12:52:56,603 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 3.0 (TID 26) (datanode3, executor 2, partition 12, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:52:56,608 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 3.0 (TID 24) in 4593 ms on datanode3 (executor 2) (8/16)\n",
      "2023-03-30 12:53:01,044 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 3.0 (TID 27) (datanode3, executor 2, partition 13, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:53:01,052 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 3.0 (TID 26) in 4449 ms on datanode3 (executor 2) (9/16)\n",
      "2023-03-30 12:53:01,315 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 3.0 (TID 28) (datanode1, executor 1, partition 8, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:53:01,319 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 3.0 (TID 25) in 4849 ms on datanode1 (executor 1) (10/16)\n",
      "2023-03-30 12:53:05,823 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 3.0 (TID 29) (datanode3, executor 2, partition 10, RACK_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:53:05,829 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 3.0 (TID 27) in 4786 ms on datanode3 (executor 2) (11/16)\n",
      "2023-03-30 12:53:06,130 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 3.0 (TID 30) (datanode1, executor 1, partition 11, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:53:06,136 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 3.0 (TID 28) in 4822 ms on datanode1 (executor 1) (12/16)\n",
      "2023-03-30 12:53:10,630 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 3.0 (TID 31) (datanode1, executor 1, partition 14, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:53:10,642 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 3.0 (TID 30) in 4513 ms on datanode1 (executor 1) (13/16)\n",
      "2023-03-30 12:53:10,777 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 3.0 (TID 29) in 4960 ms on datanode3 (executor 2) (14/16)\n",
      "2023-03-30 12:53:13,875 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 3.0 (TID 32) (datanode3, executor 2, partition 15, RACK_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:53:15,166 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 3.0 (TID 31) in 4537 ms on datanode1 (executor 1) (15/16)\n",
      "2023-03-30 12:53:18,639 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 3.0 (TID 32) in 4765 ms on datanode3 (executor 2) (16/16)\n",
      "2023-03-30 12:53:18,641 INFO scheduler.DAGScheduler: ResultStage 3 (toPandas at /tmp/ipykernel_182074/985589280.py:5) finished in 44.172 s\n",
      "2023-03-30 12:53:18,643 INFO scheduler.DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-03-30 12:53:18,643 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "2023-03-30 12:53:18,643 INFO cluster.YarnScheduler: Killing all running tasks in stage 3: Stage finished\n",
      "2023-03-30 12:53:18,644 INFO scheduler.DAGScheduler: Job 2 finished: toPandas at /tmp/ipykernel_182074/985589280.py:5, took 44.184951 s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  TimeControl           White  WhiteElo  WhiteBlunders                Black  \\\n",
       "0       300+5        dynamo21      1649             19              viviero   \n",
       "1        30+0        broskall      2290             16             BitChess   \n",
       "2       900+0  ItCouldBeWorse      1917             16            Misak-Hay   \n",
       "3        30+0           Melee      2007             16              miniond   \n",
       "4       180+0       JoseFranc      1814             15                 SSUM   \n",
       "5        30+0       daBALYAN8      1853             15            Beetlebug   \n",
       "6       300+0     TulatovOleg      1962             15             bart2008   \n",
       "7       240+2     antifragile      1428             15  AlexanderSupertramp   \n",
       "8        30+0        xxFUDOxx      1431             15            Bkmzbvfhr   \n",
       "9        60+0          mrkile      1635             14         chessrok2000   \n",
       "\n",
       "   BlackElo  BlackBlunders   Result   Termination  \n",
       "0      1705             14  1/2-1/2        Normal  \n",
       "1      2401             17      1-0  Time forfeit  \n",
       "2      1793             10      0-1        Normal  \n",
       "3      2171              9      0-1  Time forfeit  \n",
       "4      1906             15      0-1  Time forfeit  \n",
       "5      1674             12      1-0  Time forfeit  \n",
       "6      1555              8      0-1        Normal  \n",
       "7      1393              4      0-1  Time forfeit  \n",
       "8      1802              3      0-1  Time forfeit  \n",
       "9      1340             12      1-0  Time forfeit  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TimeControl</th>\n      <th>White</th>\n      <th>WhiteElo</th>\n      <th>WhiteBlunders</th>\n      <th>Black</th>\n      <th>BlackElo</th>\n      <th>BlackBlunders</th>\n      <th>Result</th>\n      <th>Termination</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>300+5</td>\n      <td>dynamo21</td>\n      <td>1649</td>\n      <td>19</td>\n      <td>viviero</td>\n      <td>1705</td>\n      <td>14</td>\n      <td>1/2-1/2</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>30+0</td>\n      <td>broskall</td>\n      <td>2290</td>\n      <td>16</td>\n      <td>BitChess</td>\n      <td>2401</td>\n      <td>17</td>\n      <td>1-0</td>\n      <td>Time forfeit</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>900+0</td>\n      <td>ItCouldBeWorse</td>\n      <td>1917</td>\n      <td>16</td>\n      <td>Misak-Hay</td>\n      <td>1793</td>\n      <td>10</td>\n      <td>0-1</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>30+0</td>\n      <td>Melee</td>\n      <td>2007</td>\n      <td>16</td>\n      <td>miniond</td>\n      <td>2171</td>\n      <td>9</td>\n      <td>0-1</td>\n      <td>Time forfeit</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>180+0</td>\n      <td>JoseFranc</td>\n      <td>1814</td>\n      <td>15</td>\n      <td>SSUM</td>\n      <td>1906</td>\n      <td>15</td>\n      <td>0-1</td>\n      <td>Time forfeit</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>30+0</td>\n      <td>daBALYAN8</td>\n      <td>1853</td>\n      <td>15</td>\n      <td>Beetlebug</td>\n      <td>1674</td>\n      <td>12</td>\n      <td>1-0</td>\n      <td>Time forfeit</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>300+0</td>\n      <td>TulatovOleg</td>\n      <td>1962</td>\n      <td>15</td>\n      <td>bart2008</td>\n      <td>1555</td>\n      <td>8</td>\n      <td>0-1</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>240+2</td>\n      <td>antifragile</td>\n      <td>1428</td>\n      <td>15</td>\n      <td>AlexanderSupertramp</td>\n      <td>1393</td>\n      <td>4</td>\n      <td>0-1</td>\n      <td>Time forfeit</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>30+0</td>\n      <td>xxFUDOxx</td>\n      <td>1431</td>\n      <td>15</td>\n      <td>Bkmzbvfhr</td>\n      <td>1802</td>\n      <td>3</td>\n      <td>0-1</td>\n      <td>Time forfeit</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>60+0</td>\n      <td>mrkile</td>\n      <td>1635</td>\n      <td>14</td>\n      <td>chessrok2000</td>\n      <td>1340</td>\n      <td>12</td>\n      <td>1-0</td>\n      <td>Time forfeit</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "eval_difference = 3.0\n",
    "eval_games = df.where(col(\"Eval\")[0].isNotNull())\n",
    "eval_games = eval_games.withColumn(\"WhiteBlunders\", (find_white_blunders(col(\"Eval\"), lit(eval_difference))))\n",
    "eval_games = eval_games.withColumn(\"BlackBlunders\", (find_black_blunders(col(\"Eval\"), lit(eval_difference))))\n",
    "eval_games.select(\"TimeControl\", \"White\", \"WhiteElo\", \"WhiteBlunders\", \"Black\", \"BlackElo\", \"BlackBlunders\", \"Result\", \"Termination\") \\\n",
    "    .orderBy(col(\"WhiteBlunders\").desc(), col(\"BlackBlunders\").desc()).limit(10).toPandas().head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot Most Blundered Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "07,810 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 11.0 (TID 95) in 4204 ms on datanode1 (executor 1) (13/16)\n",
      "2023-03-30 13:12:11,273 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 11.0 (TID 98) (datanode3, executor 2, partition 15, RACK_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 13:12:11,276 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 11.0 (TID 96) in 4402 ms on datanode3 (executor 2) (14/16)\n",
      "2023-03-30 13:12:12,466 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 11.0 (TID 97) in 4664 ms on datanode1 (executor 1) (15/16)\n",
      "2023-03-30 13:12:15,117 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 11.0 (TID 98) in 3845 ms on datanode3 (executor 2) (16/16)\n",
      "2023-03-30 13:12:15,118 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "2023-03-30 13:12:15,120 INFO scheduler.DAGScheduler: ResultStage 11 (collect at /home/ubuntu/chess_project/helper.py:45) finished in 39.874 s\n",
      "2023-03-30 13:12:15,120 INFO scheduler.DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-03-30 13:12:15,120 INFO cluster.YarnScheduler: Killing all running tasks in stage 11: Stage finished\n",
      "2023-03-30 13:12:15,121 INFO scheduler.DAGScheduler: Job 8 finished: collect at /home/ubuntu/chess_project/helper.py:45, took 39.882449 s\n",
      "2023-03-30 13:12:15,341 INFO scheduler.DAGScheduler: Registering RDD 54 (collect at /home/ubuntu/chess_project/helper.py:45) as input to shuffle 3\n",
      "2023-03-30 13:12:15,342 INFO scheduler.DAGScheduler: Got map stage job 9 (collect at /home/ubuntu/chess_project/helper.py:45) with 16 output partitions\n",
      "2023-03-30 13:12:15,342 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 12 (collect at /home/ubuntu/chess_project/helper.py:45)\n",
      "2023-03-30 13:12:15,342 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-03-30 13:12:15,342 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-03-30 13:12:15,348 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[54] at collect at /home/ubuntu/chess_project/helper.py:45), which has no missing parents\n",
      "2023-03-30 13:12:15,394 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 41.4 KiB, free 362.8 MiB)\n",
      "2023-03-30 13:12:15,401 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 362.8 MiB)\n",
      "2023-03-30 13:12:15,402 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on namenode:39703 (size: 19.9 KiB, free: 365.9 MiB)\n",
      "2023-03-30 13:12:15,403 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\n",
      "2023-03-30 13:12:15,411 INFO scheduler.DAGScheduler: Submitting 16 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[54] at collect at /home/ubuntu/chess_project/helper.py:45) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "2023-03-30 13:12:15,412 INFO cluster.YarnScheduler: Adding task set 12.0 with 16 tasks resource profile 0\n",
      "2023-03-30 13:12:15,415 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 12.0 (TID 99) (datanode1, executor 1, partition 2, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 13:12:15,415 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 100) (datanode3, executor 2, partition 0, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 13:12:15,438 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on datanode3:42711 (size: 19.9 KiB, free: 365.9 MiB)\n",
      "2023-03-30 13:12:15,438 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on datanode1:35681 (size: 19.9 KiB, free: 365.9 MiB)\n",
      "2023-03-30 13:12:19,905 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 12.0 (TID 101) (datanode1, executor 1, partition 3, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 13:12:19,907 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 12.0 (TID 99) in 4492 ms on datanode1 (executor 1) (1/16)\n",
      "2023-03-30 13:12:19,973 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 12.0 (TID 102) (datanode3, executor 2, partition 1, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 13:12:19,973 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 100) in 4558 ms on datanode3 (executor 2) (2/16)\n",
      "2023-03-30 13:12:24,402 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 12.0 (TID 103) (datanode3, executor 2, partition 6, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 13:12:24,404 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 12.0 (TID 102) in 4432 ms on datanode3 (executor 2) (3/16)\n",
      "2023-03-30 13:12:24,772 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 12.0 (TID 104) (datanode1, executor 1, partition 4, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 13:12:24,773 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 12.0 (TID 101) in 4869 ms on datanode1 (executor 1) (4/16)\n",
      "2023-03-30 13:12:29,175 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 12.0 (TID 105) (datanode1, executor 1, partition 5, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 13:12:29,178 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 12.0 (TID 104) in 4407 ms on datanode1 (executor 1) (5/16)\n",
      "2023-03-30 13:12:29,210 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 12.0 (TID 106) (datanode3, executor 2, partition 9, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 13:12:29,210 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 12.0 (TID 103) in 4809 ms on datanode3 (executor 2) (6/16)\n",
      "2023-03-30 13:12:33,745 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 12.0 (TID 107) (datanode1, executor 1, partition 7, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 13:12:33,747 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 12.0 (TID 105) in 4573 ms on datanode1 (executor 1) (7/16)\n",
      "2023-03-30 13:12:33,816 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 12.0 (TID 108) (datanode3, executor 2, partition 12, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 13:12:33,816 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 12.0 (TID 106) in 4607 ms on datanode3 (executor 2) (8/16)\n",
      "2023-03-30 13:12:37,935 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 12.0 (TID 109) (datanode3, executor 2, partition 13, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 13:12:37,940 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 12.0 (TID 108) in 4124 ms on datanode3 (executor 2) (9/16)\n",
      "2023-03-30 13:12:38,122 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 12.0 (TID 110) (datanode1, executor 1, partition 8, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 13:12:38,123 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 12.0 (TID 107) in 4379 ms on datanode1 (executor 1) (10/16)\n",
      "2023-03-30 13:12:42,499 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 12.0 (TID 111) (datanode3, executor 2, partition 10, RACK_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 13:12:42,502 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 12.0 (TID 109) in 4568 ms on datanode3 (executor 2) (11/16)\n",
      "2023-03-30 13:12:42,614 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 12.0 (TID 112) (datanode1, executor 1, partition 11, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 13:12:42,615 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 12.0 (TID 110) in 4493 ms on datanode1 (executor 1) (12/16)\n",
      "2023-03-30 13:12:46,878 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 12.0 (TID 113) (datanode1, executor 1, partition 14, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 13:12:46,879 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 12.0 (TID 112) in 4265 ms on datanode1 (executor 1) (13/16)\n",
      "2023-03-30 13:12:47,282 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 12.0 (TID 111) in 4783 ms on datanode3 (executor 2) (14/16)\n",
      "2023-03-30 13:12:50,875 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 12.0 (TID 114) (datanode3, executor 2, partition 15, RACK_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 13:12:51,264 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 12.0 (TID 113) in 4386 ms on datanode1 (executor 1) (15/16)\n",
      "2023-03-30 13:12:55,524 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 12.0 (TID 114) in 4650 ms on datanode3 (executor 2) (16/16)\n",
      "2023-03-30 13:12:55,525 INFO cluster.YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "2023-03-30 13:12:55,530 INFO scheduler.DAGScheduler: ShuffleMapStage 12 (collect at /home/ubuntu/chess_project/helper.py:45) finished in 40.166 s\n",
      "2023-03-30 13:12:55,532 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-03-30 13:12:55,532 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-03-30 13:12:55,532 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-03-30 13:12:55,532 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-03-30 13:12:55,547 INFO adaptive.ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 43756025, minimum partition size: 1048576\n",
      "2023-03-30 13:12:55,660 INFO codegen.CodeGenerator: Code generated in 61.722509 ms\n",
      "2023-03-30 13:12:55,693 INFO spark.SparkContext: Starting job: collect at /home/ubuntu/chess_project/helper.py:45\n",
      "2023-03-30 13:12:55,695 INFO scheduler.DAGScheduler: Got job 10 (collect at /home/ubuntu/chess_project/helper.py:45) with 3 output partitions\n",
      "2023-03-30 13:12:55,695 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (collect at /home/ubuntu/chess_project/helper.py:45)\n",
      "2023-03-30 13:12:55,695 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\n",
      "2023-03-30 13:12:55,695 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-03-30 13:12:55,696 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[57] at collect at /home/ubuntu/chess_project/helper.py:45), which has no missing parents\n",
      "2023-03-30 13:12:55,705 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 37.1 KiB, free 362.8 MiB)\n",
      "2023-03-30 13:12:55,715 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 362.8 MiB)\n",
      "2023-03-30 13:12:55,718 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on namenode:39703 (size: 18.6 KiB, free: 365.9 MiB)\n",
      "2023-03-30 13:12:55,719 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\n",
      "2023-03-30 13:12:55,722 INFO scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 14 (MapPartitionsRDD[57] at collect at /home/ubuntu/chess_project/helper.py:45) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "2023-03-30 13:12:55,722 INFO cluster.YarnScheduler: Adding task set 14.0 with 3 tasks resource profile 0\n",
      "2023-03-30 13:12:55,726 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 115) (datanode1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 13:12:55,727 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 14.0 (TID 116) (datanode3, executor 2, partition 1, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 13:12:55,748 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on datanode1:35681 (size: 18.6 KiB, free: 365.9 MiB)\n",
      "2023-03-30 13:12:55,757 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on datanode3:42711 (size: 18.6 KiB, free: 365.9 MiB)\n",
      "2023-03-30 13:12:55,765 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 192.168.11.172:41942\n",
      "2023-03-30 13:12:55,806 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 192.168.11.39:38512\n",
      "2023-03-30 13:12:56,441 INFO storage.BlockManagerInfo: Added taskresult_116 in memory on datanode3:42711 (size: 20.4 MiB, free: 345.4 MiB)\n",
      "2023-03-30 13:12:56,449 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 14.0 (TID 117) (datanode3, executor 2, partition 2, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 13:12:56,597 INFO client.TransportClientFactory: Successfully created connection to datanode3/192.168.11.39:42711 after 27 ms (0 ms spent in bootstraps)\n",
      "2023-03-30 13:12:56,760 INFO storage.BlockManagerInfo: Added taskresult_115 in memory on datanode1:35681 (size: 35.6 MiB, free: 330.3 MiB)\n",
      "2023-03-30 13:12:56,783 INFO client.TransportClientFactory: Successfully created connection to datanode1/192.168.11.172:35681 after 11 ms (0 ms spent in bootstraps)\n",
      "2023-03-30 13:12:57,408 INFO storage.BlockManagerInfo: Added taskresult_117 in memory on datanode3:42711 (size: 23.1 MiB, free: 322.3 MiB)\n",
      "2023-03-30 13:12:57,444 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on datanode1:35681 in memory (size: 18.9 KiB, free: 330.3 MiB)\n",
      "2023-03-30 13:12:57,446 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on namenode:39703 in memory (size: 18.9 KiB, free: 365.9 MiB)\n",
      "2023-03-30 13:12:57,447 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on datanode3:42711 in memory (size: 18.9 KiB, free: 322.3 MiB)\n",
      "2023-03-30 13:12:57,498 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on datanode1:35681 in memory (size: 24.8 KiB, free: 330.3 MiB)\n",
      "2023-03-30 13:12:57,499 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on datanode3:42711 in memory (size: 24.8 KiB, free: 322.3 MiB)\n",
      "2023-03-30 13:12:57,503 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on namenode:39703 in memory (size: 24.8 KiB, free: 365.9 MiB)\n",
      "2023-03-30 13:12:57,526 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 14.0 (TID 116) in 1800 ms on datanode3 (executor 2) (1/3)\n",
      "2023-03-30 13:12:57,531 INFO storage.BlockManagerInfo: Removed taskresult_116 on datanode3:42711 in memory (size: 20.4 MiB, free: 342.8 MiB)\n",
      "2023-03-30 13:12:57,596 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on namenode:39703 in memory (size: 25.3 KiB, free: 365.9 MiB)\n",
      "2023-03-30 13:12:57,793 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 14.0 (TID 117) in 1343 ms on datanode3 (executor 2) (2/3)\n",
      "2023-03-30 13:12:57,798 INFO storage.BlockManagerInfo: Removed taskresult_117 on datanode3:42711 in memory (size: 23.1 MiB, free: 365.9 MiB)\n",
      "2023-03-30 13:12:57,839 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on datanode1:35681 in memory (size: 25.3 KiB, free: 330.4 MiB)\n",
      "2023-03-30 13:12:57,862 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on namenode:39703 in memory (size: 19.7 KiB, free: 365.9 MiB)\n",
      "2023-03-30 13:12:57,867 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on datanode1:35681 in memory (size: 19.7 KiB, free: 330.4 MiB)\n",
      "2023-03-30 13:12:57,868 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on datanode3:42711 in memory (size: 19.7 KiB, free: 365.9 MiB)\n",
      "2023-03-30 13:12:57,901 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on datanode1:35681 in memory (size: 19.9 KiB, free: 330.4 MiB)\n",
      "2023-03-30 13:12:57,903 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on datanode3:42711 in memory (size: 19.9 KiB, free: 366.0 MiB)\n",
      "2023-03-30 13:12:57,924 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on namenode:39703 in memory (size: 19.9 KiB, free: 366.0 MiB)\n",
      "2023-03-30 13:12:57,993 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on namenode:39703 in memory (size: 53.7 KiB, free: 366.0 MiB)\n",
      "2023-03-30 13:12:57,997 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on datanode3:42711 in memory (size: 53.7 KiB, free: 366.0 MiB)\n",
      "2023-03-30 13:12:57,997 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on datanode1:35681 in memory (size: 53.7 KiB, free: 330.5 MiB)\n",
      "2023-03-30 13:12:58,439 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 115) in 2713 ms on datanode1 (executor 1) (3/3)\n",
      "2023-03-30 13:12:58,439 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "2023-03-30 13:12:58,441 INFO scheduler.DAGScheduler: ResultStage 14 (collect at /home/ubuntu/chess_project/helper.py:45) finished in 2.742 s\n",
      "2023-03-30 13:12:58,441 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-03-30 13:12:58,441 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\n",
      "2023-03-30 13:12:58,443 INFO scheduler.DAGScheduler: Job 10 finished: collect at /home/ubuntu/chess_project/helper.py:45, took 2.749899 s\n",
      "2023-03-30 13:12:58,448 INFO storage.BlockManagerInfo: Removed taskresult_115 on datanode1:35681 in memory (size: 35.6 MiB, free: 366.0 MiB)\n",
      "2023-03-30 13:13:00,266 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on datanode3:42711 in memory (size: 18.6 KiB, free: 366.0 MiB)\n",
      "2023-03-30 13:13:00,267 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on datanode1:35681 in memory (size: 18.6 KiB, free: 366.0 MiB)\n",
      "2023-03-30 13:13:00,270 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on namenode:39703 in memory (size: 18.6 KiB, free: 366.0 MiB)\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 41638)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/ubuntu/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/ubuntu/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/ubuntu/spark/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/ubuntu/spark/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/ubuntu/spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/ubuntu/spark/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "Py4JError",
     "evalue": "functions does not exist in the JVM",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_eval_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_games\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/chess_project/helper.py:46\u001b[0m, in \u001b[0;36mplot_eval_game\u001b[0;34m(eval_games)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_eval_game\u001b[39m(eval_games: pyspark\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mdataframe\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m## Get Necessary Data for Plotting...\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28meval\u001b[39m \u001b[38;5;241m=\u001b[39m eval_games\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39morderBy(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhiteBlunders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdesc())\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 46\u001b[0m     white_blunders \u001b[38;5;241m=\u001b[39m \u001b[43meval_games\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhiteBlunders\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39morderBy(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhiteBlunders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdesc())\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     47\u001b[0m     black_blunders \u001b[38;5;241m=\u001b[39m eval_games\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlackBlunders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39morderBy(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhiteBlunders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdesc())\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     48\u001b[0m     white_name \u001b[38;5;241m=\u001b[39m eval_games\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39morderBy(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhiteBlunders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdesc())\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/dataframe.py:2023\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \n\u001b[1;32m   2005\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2021\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001b[39;00m\n\u001b[1;32m   2022\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2023\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/dataframe.py:1754\u001b[0m, in \u001b[0;36mDataFrame._jcols\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cols[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m   1753\u001b[0m     cols \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_java_column\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/dataframe.py:1741\u001b[0m, in \u001b[0;36mDataFrame._jseq\u001b[0;34m(self, cols, converter)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_jseq\u001b[39m(\n\u001b[1;32m   1736\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1737\u001b[0m     cols: Sequence,\n\u001b[1;32m   1738\u001b[0m     converter: Optional[Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrimitiveType\u001b[39m\u001b[38;5;124m\"\u001b[39m, JavaObject]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1739\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JavaObject:\n\u001b[1;32m   1740\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1741\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/column.py:86\u001b[0m, in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03mConvert a list of Column (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03minto JVM Column objects.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m converter:\n\u001b[0;32m---> 86\u001b[0m     cols \u001b[38;5;241m=\u001b[39m [converter(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(cols)\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/column.py:86\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03mConvert a list of Column (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03minto JVM Column objects.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m converter:\n\u001b[0;32m---> 86\u001b[0m     cols \u001b[38;5;241m=\u001b[39m [\u001b[43mconverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(cols)\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/column.py:63\u001b[0m, in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     61\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m col\u001b[38;5;241m.\u001b[39m_jc\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m \u001b[43m_create_column_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid argument, not a string or column: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor column literals, use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstruct\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreate_map\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(col, \u001b[38;5;28mtype\u001b[39m(col))\n\u001b[1;32m     70\u001b[0m     )\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/column.py:56\u001b[0m, in \u001b[0;36m_create_column_from_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     54\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241m.\u001b[39mcol(name)\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1722\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1719\u001b[0m _, error_message \u001b[38;5;241m=\u001b[39m get_error_message(answer)\n\u001b[1;32m   1720\u001b[0m message \u001b[38;5;241m=\u001b[39m compute_exception_message(\n\u001b[1;32m   1721\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name), error_message)\n\u001b[0;32m-> 1722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(message)\n",
      "\u001b[0;31mPy4JError\u001b[0m: functions does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "plot_eval_game(eval_games)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By\n",
    "- Timecontrol ~ (60, 120, 180, 600) etc...\n",
    "- Elo-Brackets ~ ([1200, 1400], [1500, 1700], [2000-2200]) etc...\n",
    "### --> ERLEND WORK HERE YOU SCUM <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2023-03-30 12:53:20,031 INFO datasources.FileSourceStrategy: Pushed Filters: IsNotNull(Eval)\n",
      "2023-03-30 12:53:20,032 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(Eval#10),isnotnull(cast(split(translate(Eval#10, ', ), ,, -1) as array<float>)[0])\n",
      "2023-03-30 12:53:20,033 INFO datasources.FileSourceStrategy: Output Data Schema: struct<TimeControl: string, Eval: string>\n",
      "2023-03-30 12:53:20,365 INFO codegen.CodeGenerator: Code generated in 204.021701 ms\n",
      "2023-03-30 12:53:20,452 INFO codegen.CodeGenerator: Code generated in 68.115475 ms\n",
      "2023-03-30 12:53:20,462 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 485.1 KiB, free 364.7 MiB)\n",
      "2023-03-30 12:53:20,494 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 53.7 KiB, free 364.6 MiB)\n",
      "2023-03-30 12:53:20,497 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on namenode:39703 (size: 53.7 KiB, free: 366.1 MiB)\n",
      "2023-03-30 12:53:20,503 INFO spark.SparkContext: Created broadcast 5 from toPandas at /tmp/ipykernel_182074/3890739747.py:3\n",
      "2023-03-30 12:53:20,506 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-03-30 12:53:20,566 INFO scheduler.DAGScheduler: Registering RDD 21 (toPandas at /tmp/ipykernel_182074/3890739747.py:3) as input to shuffle 1\n",
      "2023-03-30 12:53:20,567 INFO scheduler.DAGScheduler: Got map stage job 3 (toPandas at /tmp/ipykernel_182074/3890739747.py:3) with 16 output partitions\n",
      "2023-03-30 12:53:20,567 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 4 (toPandas at /tmp/ipykernel_182074/3890739747.py:3)\n",
      "2023-03-30 12:53:20,567 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-03-30 12:53:20,567 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-03-30 12:53:20,571 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[21] at toPandas at /tmp/ipykernel_182074/3890739747.py:3), which has no missing parents\n",
      "2023-03-30 12:53:20,610 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 53.8 KiB, free 364.6 MiB)\n",
      "2023-03-30 12:53:20,618 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 24.8 KiB, free 364.5 MiB)\n",
      "2023-03-30 12:53:20,623 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on namenode:39703 (size: 24.8 KiB, free: 366.1 MiB)\n",
      "2023-03-30 12:53:20,625 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\n",
      "2023-03-30 12:53:20,628 INFO scheduler.DAGScheduler: Submitting 16 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[21] at toPandas at /tmp/ipykernel_182074/3890739747.py:3) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "2023-03-30 12:53:20,628 INFO cluster.YarnScheduler: Adding task set 4.0 with 16 tasks resource profile 0\n",
      "2023-03-30 12:53:20,632 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 33) (datanode3, executor 2, partition 0, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:53:20,632 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 4.0 (TID 34) (datanode1, executor 1, partition 2, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:53:20,670 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on datanode3:42711 (size: 24.8 KiB, free: 366.1 MiB)\n",
      "2023-03-30 12:53:20,670 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on datanode1:35681 (size: 24.8 KiB, free: 366.1 MiB)\n",
      "2023-03-30 12:53:20,872 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on datanode1:35681 (size: 53.7 KiB, free: 366.1 MiB)\n",
      "2023-03-30 12:53:20,921 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on datanode3:42711 (size: 53.7 KiB, free: 366.1 MiB)\n",
      "2023-03-30 12:53:25,326 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 4.0 (TID 35) (datanode1, executor 1, partition 3, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:53:25,328 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 4.0 (TID 34) in 4696 ms on datanode1 (executor 1) (1/16)\n",
      "2023-03-30 12:53:26,587 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 36) (datanode3, executor 2, partition 1, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:53:26,589 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 33) in 5958 ms on datanode3 (executor 2) (2/16)\n",
      "2023-03-30 12:53:30,033 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 4.0 (TID 37) (datanode1, executor 1, partition 4, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:53:30,036 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 4.0 (TID 35) in 4711 ms on datanode1 (executor 1) (3/16)\n",
      "2023-03-30 12:53:31,207 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 4.0 (TID 38) (datanode3, executor 2, partition 6, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:53:31,209 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 36) in 4623 ms on datanode3 (executor 2) (4/16)\n",
      "2023-03-30 12:53:35,091 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 4.0 (TID 39) (datanode1, executor 1, partition 5, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:53:35,096 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 4.0 (TID 37) in 5064 ms on datanode1 (executor 1) (5/16)\n",
      "2023-03-30 12:53:36,584 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 4.0 (TID 40) (datanode3, executor 2, partition 9, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:53:36,587 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 4.0 (TID 38) in 5381 ms on datanode3 (executor 2) (6/16)\n",
      "2023-03-30 12:53:40,111 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 4.0 (TID 41) (datanode1, executor 1, partition 7, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:53:40,114 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 4.0 (TID 39) in 5024 ms on datanode1 (executor 1) (7/16)\n",
      "2023-03-30 12:53:41,162 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 4.0 (TID 42) (datanode3, executor 2, partition 12, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:53:41,164 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 4.0 (TID 40) in 4581 ms on datanode3 (executor 2) (8/16)\n",
      "2023-03-30 12:53:44,741 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 4.0 (TID 43) (datanode1, executor 1, partition 8, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:53:44,744 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 4.0 (TID 41) in 4632 ms on datanode1 (executor 1) (9/16)\n",
      "2023-03-30 12:53:46,221 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 4.0 (TID 44) (datanode3, executor 2, partition 13, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:53:46,222 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 4.0 (TID 42) in 5061 ms on datanode3 (executor 2) (10/16)\n",
      "2023-03-30 12:53:49,336 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 4.0 (TID 45) (datanode1, executor 1, partition 10, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:53:49,340 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 4.0 (TID 43) in 4602 ms on datanode1 (executor 1) (11/16)\n",
      "2023-03-30 12:53:51,100 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 4.0 (TID 44) in 4880 ms on datanode3 (executor 2) (12/16)\n",
      "2023-03-30 12:53:52,876 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 4.0 (TID 46) (datanode3, executor 2, partition 11, RACK_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:53:54,276 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 4.0 (TID 47) (datanode1, executor 1, partition 14, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:53:54,279 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 4.0 (TID 45) in 4944 ms on datanode1 (executor 1) (13/16)\n",
      "2023-03-30 12:53:57,298 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 4.0 (TID 48) (datanode3, executor 2, partition 15, RACK_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:53:57,303 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 4.0 (TID 46) in 4428 ms on datanode3 (executor 2) (14/16)\n",
      "2023-03-30 12:53:58,931 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 4.0 (TID 47) in 4656 ms on datanode1 (executor 1) (15/16)\n",
      "2023-03-30 12:54:01,610 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 4.0 (TID 48) in 4313 ms on datanode3 (executor 2) (16/16)\n",
      "2023-03-30 12:54:01,611 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "2023-03-30 12:54:01,615 INFO scheduler.DAGScheduler: ShuffleMapStage 4 (toPandas at /tmp/ipykernel_182074/3890739747.py:3) finished in 41.037 s\n",
      "2023-03-30 12:54:01,615 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-03-30 12:54:01,616 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-03-30 12:54:01,616 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-03-30 12:54:01,616 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-03-30 12:54:01,659 INFO adaptive.ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "2023-03-30 12:54:01,739 INFO codegen.CodeGenerator: Code generated in 28.373745 ms\n",
      "2023-03-30 12:54:01,744 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-03-30 12:54:01,820 INFO codegen.CodeGenerator: Code generated in 52.534878 ms\n",
      "2023-03-30 12:54:01,882 INFO spark.SparkContext: Starting job: toPandas at /tmp/ipykernel_182074/3890739747.py:3\n",
      "2023-03-30 12:54:01,885 INFO scheduler.DAGScheduler: Got job 4 (toPandas at /tmp/ipykernel_182074/3890739747.py:3) with 1 output partitions\n",
      "2023-03-30 12:54:01,885 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (toPandas at /tmp/ipykernel_182074/3890739747.py:3)\n",
      "2023-03-30 12:54:01,885 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
      "2023-03-30 12:54:01,885 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-03-30 12:54:01,890 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[25] at toPandas at /tmp/ipykernel_182074/3890739747.py:3), which has no missing parents\n",
      "2023-03-30 12:54:01,930 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 54.3 KiB, free 364.5 MiB)\n",
      "2023-03-30 12:54:01,936 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 25.3 KiB, free 364.5 MiB)\n",
      "2023-03-30 12:54:01,938 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on namenode:39703 (size: 25.3 KiB, free: 366.1 MiB)\n",
      "2023-03-30 12:54:01,939 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\n",
      "2023-03-30 12:54:01,942 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[25] at toPandas at /tmp/ipykernel_182074/3890739747.py:3) (first 15 tasks are for partitions Vector(0))\n",
      "2023-03-30 12:54:01,942 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "2023-03-30 12:54:01,945 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 49) (datanode1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:01,971 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on datanode1:35681 (size: 25.3 KiB, free: 366.1 MiB)\n",
      "2023-03-30 12:54:02,035 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.11.172:41942\n",
      "2023-03-30 12:54:02,187 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 49) in 243 ms on datanode1 (executor 1) (1/1)\n",
      "2023-03-30 12:54:02,188 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "2023-03-30 12:54:02,190 INFO scheduler.DAGScheduler: ResultStage 6 (toPandas at /tmp/ipykernel_182074/3890739747.py:3) finished in 0.270 s\n",
      "2023-03-30 12:54:02,191 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-03-30 12:54:02,191 INFO cluster.YarnScheduler: Killing all running tasks in stage 6: Stage finished\n",
      "2023-03-30 12:54:02,192 INFO scheduler.DAGScheduler: Job 4 finished: toPandas at /tmp/ipykernel_182074/3890739747.py:3, took 0.309548 s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [TimeControl, avg(WhiteBlunders), count(TimeControl)]\n",
       "Index: []"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TimeControl</th>\n      <th>avg(WhiteBlunders)</th>\n      <th>count(TimeControl)</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "time_control_white_blunders_averages = eval_games.groupBy(\"TimeControl\").agg(mean(\"WhiteBlunders\"), count(\"TimeControl\")).withColumn(\"avg(WhiteBlunders)\", format_number(\"avg(WhiteBlunders)\", 1))\n",
    "time_control_black_blunders_averages = eval_games.groupBy(\"TimeControl\").agg(mean(\"BlackBlunders\"), count(\"TimeControl\")).withColumn(\"avg(BlackBlunders)\", format_number(\"avg(BlackBlunders)\", 1))\n",
    "time_control_white_blunders_averages.orderBy(col(\"avg(WhiteBlunders)\").desc()).where(col(\"count(TimeControl)\")>10000).limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2023-03-30 12:54:02,390 INFO datasources.FileSourceStrategy: Pushed Filters: IsNotNull(Eval)\n",
      "2023-03-30 12:54:02,391 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(Eval#10),isnotnull(cast(split(translate(Eval#10, ', ), ,, -1) as array<float>)[0])\n",
      "2023-03-30 12:54:02,392 INFO datasources.FileSourceStrategy: Output Data Schema: struct<TimeControl: string, Eval: string>\n",
      "2023-03-30 12:54:02,572 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on namenode:39703 in memory (size: 20.1 KiB, free: 366.1 MiB)\n",
      "2023-03-30 12:54:02,580 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 485.1 KiB, free 364.1 MiB)\n",
      "2023-03-30 12:54:02,595 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on datanode1:35681 in memory (size: 20.1 KiB, free: 366.1 MiB)\n",
      "2023-03-30 12:54:02,612 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on datanode3:42711 in memory (size: 20.1 KiB, free: 366.1 MiB)\n",
      "2023-03-30 12:54:02,622 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 53.7 KiB, free 364.0 MiB)\n",
      "2023-03-30 12:54:02,623 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on namenode:39703 (size: 53.7 KiB, free: 366.0 MiB)\n",
      "2023-03-30 12:54:02,625 INFO spark.SparkContext: Created broadcast 8 from toPandas at /tmp/ipykernel_182074/1036532255.py:1\n",
      "2023-03-30 12:54:02,629 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-03-30 12:54:02,652 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on datanode3:42711 in memory (size: 8.4 KiB, free: 366.1 MiB)\n",
      "2023-03-30 12:54:02,652 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on datanode1:35681 in memory (size: 8.4 KiB, free: 366.1 MiB)\n",
      "2023-03-30 12:54:02,660 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on namenode:39703 in memory (size: 8.4 KiB, free: 366.0 MiB)\n",
      "2023-03-30 12:54:02,672 INFO scheduler.DAGScheduler: Registering RDD 32 (toPandas at /tmp/ipykernel_182074/1036532255.py:1) as input to shuffle 2\n",
      "2023-03-30 12:54:02,672 INFO scheduler.DAGScheduler: Got map stage job 5 (toPandas at /tmp/ipykernel_182074/1036532255.py:1) with 16 output partitions\n",
      "2023-03-30 12:54:02,672 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 7 (toPandas at /tmp/ipykernel_182074/1036532255.py:1)\n",
      "2023-03-30 12:54:02,672 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-03-30 12:54:02,673 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-03-30 12:54:02,679 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[32] at toPandas at /tmp/ipykernel_182074/1036532255.py:1), which has no missing parents\n",
      "2023-03-30 12:54:02,701 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 53.8 KiB, free 364.0 MiB)\n",
      "2023-03-30 12:54:02,706 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 24.8 KiB, free 363.9 MiB)\n",
      "2023-03-30 12:54:02,707 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on namenode:39703 (size: 24.8 KiB, free: 366.0 MiB)\n",
      "2023-03-30 12:54:02,709 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\n",
      "2023-03-30 12:54:02,710 INFO scheduler.DAGScheduler: Submitting 16 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[32] at toPandas at /tmp/ipykernel_182074/1036532255.py:1) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "2023-03-30 12:54:02,710 INFO cluster.YarnScheduler: Adding task set 7.0 with 16 tasks resource profile 0\n",
      "2023-03-30 12:54:02,713 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 7.0 (TID 50) (datanode1, executor 1, partition 2, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:02,713 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 51) (datanode3, executor 2, partition 0, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:02,717 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on datanode1:35681 in memory (size: 5.5 KiB, free: 366.1 MiB)\n",
      "2023-03-30 12:54:02,717 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on namenode:39703 in memory (size: 5.5 KiB, free: 366.0 MiB)\n",
      "2023-03-30 12:54:02,728 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on namenode:39703 in memory (size: 25.3 KiB, free: 366.0 MiB)\n",
      "2023-03-30 12:54:02,738 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on datanode1:35681 in memory (size: 25.3 KiB, free: 366.1 MiB)\n",
      "2023-03-30 12:54:02,744 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on datanode1:35681 (size: 24.8 KiB, free: 366.1 MiB)\n",
      "2023-03-30 12:54:02,750 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on datanode3:42711 (size: 24.8 KiB, free: 366.1 MiB)\n",
      "2023-03-30 12:54:02,760 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on namenode:39703 in memory (size: 24.8 KiB, free: 366.1 MiB)\n",
      "2023-03-30 12:54:02,761 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on datanode1:35681 in memory (size: 24.8 KiB, free: 366.1 MiB)\n",
      "2023-03-30 12:54:02,762 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on datanode3:42711 in memory (size: 24.8 KiB, free: 366.1 MiB)\n",
      "2023-03-30 12:54:02,781 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on datanode1:35681 (size: 53.7 KiB, free: 366.1 MiB)\n",
      "2023-03-30 12:54:02,842 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on datanode3:42711 (size: 53.7 KiB, free: 366.1 MiB)\n",
      "2023-03-30 12:54:07,253 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 52) (datanode3, executor 2, partition 1, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:07,256 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 51) in 4543 ms on datanode3 (executor 2) (1/16)\n",
      "2023-03-30 12:54:07,839 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 7.0 (TID 53) (datanode1, executor 1, partition 3, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:07,841 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 7.0 (TID 50) in 5128 ms on datanode1 (executor 1) (2/16)\n",
      "2023-03-30 12:54:11,755 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 7.0 (TID 54) (datanode3, executor 2, partition 6, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:11,756 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 7.0 (TID 52) in 4504 ms on datanode3 (executor 2) (3/16)\n",
      "2023-03-30 12:54:12,551 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 7.0 (TID 55) (datanode1, executor 1, partition 4, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:12,552 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 7.0 (TID 53) in 4714 ms on datanode1 (executor 1) (4/16)\n",
      "2023-03-30 12:54:16,249 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 7.0 (TID 56) (datanode3, executor 2, partition 9, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:16,251 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 7.0 (TID 54) in 4496 ms on datanode3 (executor 2) (5/16)\n",
      "2023-03-30 12:54:17,330 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 7.0 (TID 57) (datanode1, executor 1, partition 5, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:17,331 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 7.0 (TID 55) in 4781 ms on datanode1 (executor 1) (6/16)\n",
      "2023-03-30 12:54:20,591 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 7.0 (TID 58) (datanode3, executor 2, partition 12, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:20,592 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 7.0 (TID 56) in 4343 ms on datanode3 (executor 2) (7/16)\n",
      "2023-03-30 12:54:21,744 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 7.0 (TID 59) (datanode1, executor 1, partition 7, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:21,745 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 7.0 (TID 57) in 4415 ms on datanode1 (executor 1) (8/16)\n",
      "2023-03-30 12:54:25,020 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 7.0 (TID 60) (datanode3, executor 2, partition 13, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:25,022 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 7.0 (TID 58) in 4431 ms on datanode3 (executor 2) (9/16)\n",
      "2023-03-30 12:54:26,295 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 7.0 (TID 61) (datanode1, executor 1, partition 8, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:26,295 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 7.0 (TID 59) in 4553 ms on datanode1 (executor 1) (10/16)\n",
      "2023-03-30 12:54:29,459 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 7.0 (TID 62) (datanode3, executor 2, partition 10, RACK_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:29,461 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 7.0 (TID 60) in 4441 ms on datanode3 (executor 2) (11/16)\n",
      "2023-03-30 12:54:31,653 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 7.0 (TID 63) (datanode1, executor 1, partition 11, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:31,655 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 7.0 (TID 61) in 5361 ms on datanode1 (executor 1) (12/16)\n",
      "2023-03-30 12:54:33,507 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 7.0 (TID 62) in 4048 ms on datanode3 (executor 2) (13/16)\n",
      "2023-03-30 12:54:34,875 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 7.0 (TID 64) (datanode3, executor 2, partition 14, RACK_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:36,206 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 7.0 (TID 65) (datanode1, executor 1, partition 15, NODE_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:36,208 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 7.0 (TID 63) in 4556 ms on datanode1 (executor 1) (14/16)\n",
      "2023-03-30 12:54:39,266 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 7.0 (TID 64) in 4392 ms on datanode3 (executor 2) (15/16)\n",
      "2023-03-30 12:54:40,531 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 7.0 (TID 65) in 4325 ms on datanode1 (executor 1) (16/16)\n",
      "2023-03-30 12:54:40,531 INFO cluster.YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "2023-03-30 12:54:40,533 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (toPandas at /tmp/ipykernel_182074/1036532255.py:1) finished in 37.847 s\n",
      "2023-03-30 12:54:40,534 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-03-30 12:54:40,534 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-03-30 12:54:40,534 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-03-30 12:54:40,534 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-03-30 12:54:40,553 INFO adaptive.ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "2023-03-30 12:54:40,572 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-03-30 12:54:40,622 INFO spark.SparkContext: Starting job: toPandas at /tmp/ipykernel_182074/1036532255.py:1\n",
      "2023-03-30 12:54:40,624 INFO scheduler.DAGScheduler: Got job 6 (toPandas at /tmp/ipykernel_182074/1036532255.py:1) with 1 output partitions\n",
      "2023-03-30 12:54:40,624 INFO scheduler.DAGScheduler: Final stage: ResultStage 9 (toPandas at /tmp/ipykernel_182074/1036532255.py:1)\n",
      "2023-03-30 12:54:40,624 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
      "2023-03-30 12:54:40,624 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-03-30 12:54:40,625 INFO scheduler.DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[36] at toPandas at /tmp/ipykernel_182074/1036532255.py:1), which has no missing parents\n",
      "2023-03-30 12:54:40,662 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 54.3 KiB, free 364.1 MiB)\n",
      "2023-03-30 12:54:40,675 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 25.3 KiB, free 364.0 MiB)\n",
      "2023-03-30 12:54:40,676 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on namenode:39703 (size: 25.3 KiB, free: 366.0 MiB)\n",
      "2023-03-30 12:54:40,677 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\n",
      "2023-03-30 12:54:40,679 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[36] at toPandas at /tmp/ipykernel_182074/1036532255.py:1) (first 15 tasks are for partitions Vector(0))\n",
      "2023-03-30 12:54:40,679 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "2023-03-30 12:54:40,681 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 66) (datanode1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:40,704 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on datanode1:35681 (size: 25.3 KiB, free: 366.0 MiB)\n",
      "2023-03-30 12:54:40,716 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 192.168.11.172:41942\n",
      "2023-03-30 12:54:40,778 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 66) in 96 ms on datanode1 (executor 1) (1/1)\n",
      "2023-03-30 12:54:40,778 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "2023-03-30 12:54:40,780 INFO scheduler.DAGScheduler: ResultStage 9 (toPandas at /tmp/ipykernel_182074/1036532255.py:1) finished in 0.134 s\n",
      "2023-03-30 12:54:40,781 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-03-30 12:54:40,781 INFO cluster.YarnScheduler: Killing all running tasks in stage 9: Stage finished\n",
      "2023-03-30 12:54:40,781 INFO scheduler.DAGScheduler: Job 6 finished: toPandas at /tmp/ipykernel_182074/1036532255.py:1, took 0.158842 s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [TimeControl, avg(BlackBlunders), count(TimeControl)]\n",
       "Index: []"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TimeControl</th>\n      <th>avg(BlackBlunders)</th>\n      <th>count(TimeControl)</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "time_control_black_blunders_averages.orderBy(col(\"avg(BlackBlunders)\").desc()).where(col(\"count(TimeControl)\")>100000).limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2023-03-30 12:54:40,999 INFO datasources.FileSourceStrategy: Pushed Filters: IsNotNull(Eval)\n",
      "2023-03-30 12:54:41,001 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(Eval#10),isnotnull(cast(split(translate(Eval#10, ', ), ,, -1) as array<float>)[0])\n",
      "2023-03-30 12:54:41,001 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Moves: string, Eval: string>\n",
      "2023-03-30 12:54:41,069 INFO codegen.CodeGenerator: Code generated in 43.037777 ms\n",
      "2023-03-30 12:54:41,126 INFO codegen.CodeGenerator: Code generated in 42.395952 ms\n",
      "2023-03-30 12:54:41,226 INFO codegen.CodeGenerator: Code generated in 78.985482 ms\n",
      "2023-03-30 12:54:41,233 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 485.1 KiB, free 363.6 MiB)\n",
      "2023-03-30 12:54:41,260 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 53.7 KiB, free 363.5 MiB)\n",
      "2023-03-30 12:54:41,262 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on namenode:39703 (size: 53.7 KiB, free: 366.0 MiB)\n",
      "2023-03-30 12:54:41,264 INFO spark.SparkContext: Created broadcast 11 from take at /tmp/ipykernel_182074/3532045529.py:1\n",
      "2023-03-30 12:54:41,268 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-03-30 12:54:41,313 INFO spark.SparkContext: Starting job: take at /tmp/ipykernel_182074/3532045529.py:1\n",
      "2023-03-30 12:54:41,315 INFO scheduler.DAGScheduler: Got job 7 (take at /tmp/ipykernel_182074/3532045529.py:1) with 16 output partitions\n",
      "2023-03-30 12:54:41,315 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (take at /tmp/ipykernel_182074/3532045529.py:1)\n",
      "2023-03-30 12:54:41,315 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-03-30 12:54:41,315 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-03-30 12:54:41,316 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[44] at take at /tmp/ipykernel_182074/3532045529.py:1), which has no missing parents\n",
      "2023-03-30 12:54:41,341 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 42.8 KiB, free 363.5 MiB)\n",
      "2023-03-30 12:54:41,346 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 19.7 KiB, free 363.5 MiB)\n",
      "2023-03-30 12:54:41,347 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on namenode:39703 (size: 19.7 KiB, free: 366.0 MiB)\n",
      "2023-03-30 12:54:41,351 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\n",
      "2023-03-30 12:54:41,352 INFO scheduler.DAGScheduler: Submitting 16 missing tasks from ResultStage 10 (MapPartitionsRDD[44] at take at /tmp/ipykernel_182074/3532045529.py:1) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "2023-03-30 12:54:41,352 INFO cluster.YarnScheduler: Adding task set 10.0 with 16 tasks resource profile 0\n",
      "2023-03-30 12:54:41,354 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 67) (datanode3, executor 2, partition 0, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:41,354 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 10.0 (TID 68) (datanode1, executor 1, partition 2, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:41,374 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on datanode3:42711 (size: 19.7 KiB, free: 366.0 MiB)\n",
      "2023-03-30 12:54:41,375 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on datanode1:35681 (size: 19.7 KiB, free: 366.0 MiB)\n",
      "2023-03-30 12:54:41,466 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on datanode1:35681 (size: 53.7 KiB, free: 366.0 MiB)\n",
      "2023-03-30 12:54:41,475 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on datanode3:42711 (size: 53.7 KiB, free: 366.0 MiB)\n",
      "2023-03-30 12:54:46,498 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 10.0 (TID 69) (datanode3, executor 2, partition 1, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:46,500 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 67) in 5146 ms on datanode3 (executor 2) (1/16)\n",
      "2023-03-30 12:54:46,800 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 10.0 (TID 70) (datanode1, executor 1, partition 3, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:46,802 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 10.0 (TID 68) in 5448 ms on datanode1 (executor 1) (2/16)\n",
      "2023-03-30 12:54:52,295 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 10.0 (TID 71) (datanode3, executor 2, partition 6, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:52,300 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 10.0 (TID 69) in 5803 ms on datanode3 (executor 2) (3/16)\n",
      "2023-03-30 12:54:52,545 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 10.0 (TID 72) (datanode1, executor 1, partition 4, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:52,551 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 10.0 (TID 70) in 5751 ms on datanode1 (executor 1) (4/16)\n",
      "2023-03-30 12:54:57,254 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 10.0 (TID 71) in 4960 ms on datanode3 (executor 2) (5/16)\n",
      "2023-03-30 12:54:57,262 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 10.0 (TID 73) (datanode3, executor 2, partition 9, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:58,748 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 10.0 (TID 74) (datanode1, executor 1, partition 5, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:54:58,753 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 10.0 (TID 72) in 6209 ms on datanode1 (executor 1) (6/16)\n",
      "2023-03-30 12:55:02,472 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 10.0 (TID 75) (datanode3, executor 2, partition 12, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:55:02,479 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 10.0 (TID 73) in 5218 ms on datanode3 (executor 2) (7/16)\n",
      "2023-03-30 12:55:03,594 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 10.0 (TID 76) (datanode1, executor 1, partition 7, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:55:03,598 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 10.0 (TID 74) in 4850 ms on datanode1 (executor 1) (8/16)\n",
      "2023-03-30 12:55:07,379 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 10.0 (TID 77) (datanode3, executor 2, partition 13, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:55:07,385 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 10.0 (TID 75) in 4913 ms on datanode3 (executor 2) (9/16)\n",
      "2023-03-30 12:55:08,893 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 10.0 (TID 78) (datanode1, executor 1, partition 8, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:55:08,898 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 10.0 (TID 76) in 5305 ms on datanode1 (executor 1) (10/16)\n",
      "2023-03-30 12:55:12,388 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 10.0 (TID 79) (datanode3, executor 2, partition 10, RACK_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:55:12,393 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 10.0 (TID 77) in 5015 ms on datanode3 (executor 2) (11/16)\n",
      "2023-03-30 12:55:14,625 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 10.0 (TID 80) (datanode1, executor 1, partition 11, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:55:14,629 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 10.0 (TID 78) in 5737 ms on datanode1 (executor 1) (12/16)\n",
      "2023-03-30 12:55:17,046 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 10.0 (TID 79) in 4658 ms on datanode3 (executor 2) (13/16)\n",
      "2023-03-30 12:55:17,875 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 10.0 (TID 81) (datanode3, executor 2, partition 14, RACK_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:55:19,857 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 10.0 (TID 82) (datanode1, executor 1, partition 15, NODE_LOCAL, 4939 bytes) taskResourceAssignments Map()\n",
      "2023-03-30 12:55:19,862 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 10.0 (TID 80) in 5237 ms on datanode1 (executor 1) (14/16)\n",
      "2023-03-30 12:55:23,167 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 10.0 (TID 81) in 5293 ms on datanode3 (executor 2) (15/16)\n",
      "['e4', ' e5', ' Nf3', ' Nc6', ' Bc4', ' h6', ' d3', ' Nf6', ' Nc3', ' d6', ' Be3', ' Bg4', ' Qe2', ' a6', ' h3', ' Bxf3', ' Qxf3', ' b5', ' Nd5', ' bxc4', ' Nxf6+', ' Qxf6', ' Qxf6', ' gxf6', ' dxc4', ' Nb4', ' O-O-O', ' Nxa2+', ' Kb1', ' Nb4', ' c3', ' Nc6', ' f4', ' Bg7', ' f5', ' O-O', ' h4', ' Rfb8', ' g4', ' Na5', ' c5', ' Bf8', ' cxd6', ' Bxd6', ' g5', ' Nc4', ' Bc1', ' Ba3', ' Ka2', ' Bxb2', ' Bxb2', ' Rxb2+', ' Ka1', ' Kg7', ' gxh6+', ' Kxh6', ' Rhg1', ' Re2', ' Rd7', ' Rf8', ' Rxc7', ' Ne3', ' Rb1', ' Nc2+', ' Ka2', ' Ne3+', ' Ka3', ' Kh5', ' Rbb7', ' Nc2+', ' Ka4', ' Kxh4', ' Rxf7', ' Rxf7', ' Rxf7', ' Kg5', ' c4', ' Rxe4', ' Ka5', ' Rxc4', ' Kxa6', ' Ra4+', ' Kb5', ' Rb4+', ' Kc5', ' Rf4', ' Kd6', ' Kxf5', ' Ke7', ' Ke4', ' Ke6', ' Nd4+', ' Kd6', ' Kf5', ' Kd5', ' Nb5', ' Rb7', ' Nc3+', ' Kd6', ' Ne4+', ' Ke7', ' Nc5', ' Rb6', ' e4', ' Rxf6+', ' Kg5', ' Rc6', ' Nd3', ' Rc4', ' Nf2', ' Ke6', ' Kg4', ' Rc3', ' Nd1', ' Rc2', ' Rf3', ' Ke5', ' e3', ' Re2', ' Nc3', ' Kd4', ' Nd1', ' Kd3', ' Kg3', ' Kc2', ' Nf2', ' Kc3', ' Kf4', ' Kd4', ' Nd1', ' Re1', ' Nf2', ' Re2', ' Ne4', ' Kd3', ' Ng3', ' Re1', ' Ne4', ' Ke2', ' Nc3+', ' Kd3', ' Ne4', ' Kd4', ' Nd6', ' Re2', ' Nb5+', ' Kd3', ' Nd6', ' Kd4', ' Nf5+', ' Kd3', ' Ng3', ' Re1', ' Nf5', ' Re2', ' Nd6', ' Re1', ' Nb5', ' Re2', ' Na3', ' Re1', ' Nc2', ' Re2', ' Nb4+', ' Kd4', ' Nc6+', ' Kd3', ' Ne7', ' Kd4', ' Nf5+', ' Kd3', ' Rf2', ' Re1', ' Kf3', ' Rg1', ' e2', ' Re1', ' Rf1', ' Rxe2', ' Ng3', ' Re8', ' Rd1+', ' Kc2', ' Rd7', ' Rf8+', ' Kg2', ' Rg8', ' Rd4', ' Kc3', ' Rd5', ' Kc4', ' Rd1', ' Kc3', ' Kf3', ' Rh8', ' Rd7', ' Rh3', ' Kg4', ' Rh8', ' Ne4+']\n",
      "2023-03-30 12:55:24,757 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 10.0 (TID 82) in 4901 ms on datanode1 (executor 1) (16/16)\n",
      "2023-03-30 12:55:24,757 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "2023-03-30 12:55:24,759 INFO scheduler.DAGScheduler: ResultStage 10 (take at /tmp/ipykernel_182074/3532045529.py:1) finished in 43.439 s\n",
      "2023-03-30 12:55:24,759 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-03-30 12:55:24,759 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\n",
      "2023-03-30 12:55:24,760 INFO scheduler.DAGScheduler: Job 7 finished: take at /tmp/ipykernel_182074/3532045529.py:1, took 43.446713 s\n",
      "2023-03-30 12:55:24,808 INFO codegen.CodeGenerator: Code generated in 29.501072 ms\n"
     ]
    }
   ],
   "source": [
    "a = eval_games.select(\"Moves\") \\\n",
    "    .orderBy(col(\"WhiteBlunders\").desc(), col(\"BlackBlunders\").desc()).limit(1).take(1)[0][0]\n",
    "print([x.replace(\"'\",\"\").replace('\"', \"\").strip(\"'\") for x in a])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}