{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 11:01:59,957 INFO spark.SparkContext: Running Spark version 3.3.2\n",
      "2023-03-31 11:02:00,358 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-03-31 11:02:00,359 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2023-03-31 11:02:00,359 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-03-31 11:02:00,360 INFO spark.SparkContext: Submitted application: test\n",
      "2023-03-31 11:02:00,394 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2023-03-31 11:02:00,418 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "2023-03-31 11:02:00,421 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2023-03-31 11:02:00,517 INFO spark.SecurityManager: Changing view acls to: ubuntu\n",
      "2023-03-31 11:02:00,518 INFO spark.SecurityManager: Changing modify acls to: ubuntu\n",
      "2023-03-31 11:02:00,519 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-03-31 11:02:00,520 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-03-31 11:02:00,521 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()\n",
      "2023-03-31 11:02:00,999 INFO util.Utils: Successfully started service 'sparkDriver' on port 41153.\n",
      "2023-03-31 11:02:01,054 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2023-03-31 11:02:01,124 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2023-03-31 11:02:01,158 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2023-03-31 11:02:01,159 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2023-03-31 11:02:01,206 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2023-03-31 11:02:01,247 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-c2cc5c79-a51b-4c62-8db6-54074b3a6486\n",
      "2023-03-31 11:02:01,275 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "2023-03-31 11:02:01,326 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2023-03-31 11:02:01,396 INFO util.log: Logging initialized @4004ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2023-03-31 11:02:01,627 INFO server.Server: jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09\n",
      "2023-03-31 11:02:01,662 INFO server.Server: Started @4272ms\n",
      "2023-03-31 11:02:01,706 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2023-03-31 11:02:01,725 INFO server.AbstractConnector: Started ServerConnector@659a42ca{HTTP/1.1, (http/1.1)}{0.0.0.0:4041}\n",
      "2023-03-31 11:02:01,725 INFO util.Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "2023-03-31 11:02:01,771 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fcd6cdf{/,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:02,645 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at namenode/192.168.11.141:8032\n",
      "2023-03-31 11:02:04,310 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-03-31 11:02:04,311 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-03-31 11:02:04,347 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)\n",
      "2023-03-31 11:02:04,350 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "2023-03-31 11:02:04,351 INFO yarn.Client: Setting up container launch context for our AM\n",
      "2023-03-31 11:02:04,356 INFO yarn.Client: Setting up the launch environment for our AM container\n",
      "2023-03-31 11:02:04,367 INFO yarn.Client: Preparing resources for our AM container\n",
      "2023-03-31 11:02:04,460 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "2023-03-31 11:02:08,074 INFO yarn.Client: Uploading resource file:/tmp/spark-67318b99-80e0-4954-bb07-bdc7e2db11f9/__spark_libs__3427352440851377475.zip -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1678881332295_0036/__spark_libs__3427352440851377475.zip\n",
      "2023-03-31 11:02:09,770 INFO yarn.Client: Uploading resource file:/home/ubuntu/spark/python/lib/pyspark.zip -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1678881332295_0036/pyspark.zip\n",
      "2023-03-31 11:02:09,888 INFO yarn.Client: Uploading resource file:/home/ubuntu/spark/python/lib/py4j-0.10.9.5-src.zip -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1678881332295_0036/py4j-0.10.9.5-src.zip\n",
      "2023-03-31 11:02:10,189 INFO yarn.Client: Uploading resource file:/tmp/spark-67318b99-80e0-4954-bb07-bdc7e2db11f9/__spark_conf__5576697501910374915.zip -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1678881332295_0036/__spark_conf__.zip\n",
      "2023-03-31 11:02:10,298 INFO spark.SecurityManager: Changing view acls to: ubuntu\n",
      "2023-03-31 11:02:10,298 INFO spark.SecurityManager: Changing modify acls to: ubuntu\n",
      "2023-03-31 11:02:10,299 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-03-31 11:02:10,299 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-03-31 11:02:10,299 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()\n",
      "2023-03-31 11:02:10,344 INFO yarn.Client: Submitting application application_1678881332295_0036 to ResourceManager\n",
      "2023-03-31 11:02:10,634 INFO impl.YarnClientImpl: Submitted application application_1678881332295_0036\n",
      "2023-03-31 11:02:11,640 INFO yarn.Client: Application report for application_1678881332295_0036 (state: ACCEPTED)\n",
      "2023-03-31 11:02:11,647 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1680260530395\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://namenode:8088/proxy/application_1678881332295_0036/\n",
      "\t user: ubuntu\n",
      "2023-03-31 11:02:12,650 INFO yarn.Client: Application report for application_1678881332295_0036 (state: ACCEPTED)\n",
      "2023-03-31 11:02:13,654 INFO yarn.Client: Application report for application_1678881332295_0036 (state: ACCEPTED)\n",
      "2023-03-31 11:02:14,657 INFO yarn.Client: Application report for application_1678881332295_0036 (state: ACCEPTED)\n",
      "2023-03-31 11:02:15,660 INFO yarn.Client: Application report for application_1678881332295_0036 (state: ACCEPTED)\n",
      "2023-03-31 11:02:16,619 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> namenode, PROXY_URI_BASES -> http://namenode:8088/proxy/application_1678881332295_0036), /proxy/application_1678881332295_0036\n",
      "2023-03-31 11:02:16,662 INFO yarn.Client: Application report for application_1678881332295_0036 (state: RUNNING)\n",
      "2023-03-31 11:02:16,663 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 192.168.11.39\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1680260530395\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://namenode:8088/proxy/application_1678881332295_0036/\n",
      "\t user: ubuntu\n",
      "2023-03-31 11:02:16,665 INFO cluster.YarnClientSchedulerBackend: Application application_1678881332295_0036 has started running.\n",
      "2023-03-31 11:02:16,686 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37243.\n",
      "2023-03-31 11:02:16,687 INFO netty.NettyBlockTransferService: Server created on namenode:37243\n",
      "2023-03-31 11:02:16,689 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2023-03-31 11:02:16,706 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, namenode, 37243, None)\n",
      "2023-03-31 11:02:16,711 INFO storage.BlockManagerMasterEndpoint: Registering block manager namenode:37243 with 366.3 MiB RAM, BlockManagerId(driver, namenode, 37243, None)\n",
      "2023-03-31 11:02:16,719 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, namenode, 37243, None)\n",
      "2023-03-31 11:02:16,720 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, namenode, 37243, None)\n",
      "2023-03-31 11:02:17,027 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5fcd6cdf{/,null,STOPPED,@Spark}\n",
      "2023-03-31 11:02:17,032 INFO ui.ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,040 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6cf68c23{/jobs,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,040 INFO ui.ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,042 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58613240{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,043 INFO ui.ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,045 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@336a3ec6{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,045 INFO ui.ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,046 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5bfcba1b{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,047 INFO ui.ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,048 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@674a5fa{/stages,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,048 INFO ui.ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,049 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c62c322{/stages/json,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,050 INFO ui.ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,051 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3b661e0f{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,052 INFO ui.ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,053 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@461d6da6{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,053 INFO ui.ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,055 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7915fb45{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,055 INFO ui.ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,056 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@a30dfa0{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,056 INFO ui.ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,058 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d44a461{/storage,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,058 INFO ui.ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,063 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fa0a44b{/storage/json,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,063 INFO ui.ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,065 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2252e19a{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,065 INFO ui.ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,067 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7599c5f2{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,067 INFO ui.ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,069 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@498752ed{/environment,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,069 INFO ui.ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,070 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3162b3ba{/environment/json,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,071 INFO ui.ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,072 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@12abb634{/executors,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,073 INFO ui.ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,074 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64862c15{/executors/json,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,074 INFO ui.ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,076 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a576575{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,077 INFO ui.ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,079 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e06d34c{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,080 INFO ui.ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,097 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c96b5c7{/static,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,098 INFO ui.ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,100 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a17a775{/,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,101 INFO ui.ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,106 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29a25eb6{/api,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,107 INFO ui.ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,112 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7ebbab5{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,113 INFO ui.ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,114 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7564c9cb{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:17,121 INFO ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:17,123 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70c1f276{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:18,260 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "2023-03-31 11:02:22,679 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.11.39:36840) with ID 1,  ResourceProfileId 0\n",
      "2023-03-31 11:02:22,853 INFO storage.BlockManagerMasterEndpoint: Registering block manager datanode3:36893 with 366.3 MiB RAM, BlockManagerId(1, datanode3, 36893, None)\n",
      "2023-03-31 11:02:24,391 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.11.172:51818) with ID 2,  ResourceProfileId 0\n",
      "2023-03-31 11:02:24,452 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "2023-03-31 11:02:24,546 INFO storage.BlockManagerMasterEndpoint: Registering block manager datanode1:32893 with 366.3 MiB RAM, BlockManagerId(2, datanode1, 32893, None)\n",
      "2023-03-31 11:02:24,795 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "2023-03-31 11:02:24,800 INFO internal.SharedState: Warehouse path is 'file:/home/ubuntu/chess_project/spark-warehouse'.\n",
      "2023-03-31 11:02:24,820 INFO ui.ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:24,827 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@12717918{/SQL,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:24,827 INFO ui.ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:24,828 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@160dd088{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:24,828 INFO ui.ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:24,829 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f1f23f4{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:24,830 INFO ui.ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:24,831 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@fd3f1f9{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:24,831 INFO ui.ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-03-31 11:02:24,833 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b62f9cc{/static/sql,null,AVAILABLE,@Spark}\n",
      "2023-03-31 11:02:26,488 INFO datasources.InMemoryFileIndex: It took 116 ms to list leaf files for 1 paths.\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries #\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql.functions import split, col, array_contains, translate, round, size, when, udf, lit, mean, count, format_number\n",
    "from pyspark.sql.types import TimestampType, MapType, IntegerType, StringType, ArrayType, FloatType, StructField, StructType\n",
    "from pyspark.sql import SparkSession\n",
    "from helper import *\n",
    "\n",
    "spark = SparkSession.builder.appName('test').master(\"yarn\").getOrCreate()\n",
    "## MUTE OUTPUT FROM SPARK\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "#Event, White, Black, Result, WhiteElo, BlackElo, Opening, TimeControl, Termination, Moves,Eval, UTCTimestamp\n",
    "schema = StructType([ \\\n",
    "    StructField(\"Event\",StringType(),True), \\\n",
    "    StructField(\"White\",StringType(),True), \\\n",
    "    StructField(\"Black\",StringType(),True), \\\n",
    "    StructField(\"Result\", StringType(), True), \\\n",
    "    StructField(\"WhiteElo\", IntegerType(), True), \\\n",
    "    StructField(\"BlackElo\", IntegerType(), True), \\\n",
    "    StructField(\"Opening\",StringType(),True), \\\n",
    "    StructField(\"TimeControl\",StringType(),True), \\\n",
    "    StructField(\"Termination\",StringType(),True), \\\n",
    "    StructField(\"Moves\", StringType(), True), \\\n",
    "    StructField(\"Eval\", StringType(), True), \\\n",
    "    StructField(\"UTCTimestamp\", TimestampType(), True) \\\n",
    "  ])\n",
    "df = spark.read.csv(\"hdfs://namenode:9000/chess_2016_dataset/output/part*\", schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check Shape of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"shape: \", (df.count(), len(df.columns)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert columns to appropriate types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convert_types(df)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_difference = 3.0\n",
    "eval_games = df.where(col(\"Eval\")[0].isNotNull())\n",
    "eval_games = eval_games.withColumn(\"WhiteBlunders\", (find_white_blunders(col(\"Eval\"), lit(eval_difference))))\n",
    "eval_games = eval_games.withColumn(\"BlackBlunders\", (find_black_blunders(col(\"Eval\"), lit(eval_difference))))\n",
    "eval_games.select(\"TimeControl\", \"White\", \"WhiteElo\", \"WhiteBlunders\", \"Black\", \"BlackElo\", \"BlackBlunders\", \"Result\", \"Termination\") \\\n",
    "    .orderBy(col(\"WhiteBlunders\").desc(), col(\"BlackBlunders\").desc()).limit(10).toPandas().head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot Most Blundered Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "plot_eval_game(eval_games)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By\n",
    "- Timecontrol ~ (60, 120, 180, 600) etc...\n",
    "- Elo-Brackets ~ ([1200, 1400], [1500, 1700], [2000-2200]) etc...\n",
    "### --> ERLEND WORK HERE YOU SCUM <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_control_white_blunders_averages = eval_games.groupBy(\"TimeControl\").agg(mean(\"WhiteBlunders\"), count(\"TimeControl\")).withColumn(\"avg(WhiteBlunders)\", format_number(\"avg(WhiteBlunders)\", 1))\n",
    "time_control_black_blunders_averages = eval_games.groupBy(\"TimeControl\").agg(mean(\"BlackBlunders\"), count(\"TimeControl\")).withColumn(\"avg(BlackBlunders)\", format_number(\"avg(BlackBlunders)\", 1))\n",
    "time_control_white_blunders_averages.orderBy(col(\"avg(WhiteBlunders)\").desc()).where(col(\"count(TimeControl)\")>10000).limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_control_black_blunders_averages.orderBy(col(\"avg(BlackBlunders)\").desc()).where(col(\"count(TimeControl)\")>100000).limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = eval_games.select(\"Moves\") \\\n",
    "    .orderBy(col(\"WhiteBlunders\").desc(), col(\"BlackBlunders\").desc()).limit(1).take(1)[0][0]\n",
    "print([x.replace(\"'\",\"\").replace('\"', \"\").strip(\"'\") for x in a])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
